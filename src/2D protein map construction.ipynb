{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "predicted_binding_sites_folder = 'predicted_binding_sites'\n",
    "\n",
    "prediction_files = list(filter(lambda file: '_predictions' in file , os.listdir(predicted_binding_sites_folder)))\n",
    "\n",
    "print(len(prediction_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(os.path.join(predicted_binding_sites_folder, prediction_files[0]))\n",
    "\n",
    "#df.iloc[0,9].strip().split(' ') # list\n",
    "\n",
    "#[token.split('_')[-1] for token in df.iloc[0,9].strip().split(' ')] #residue ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A_105',\n",
       " 'A_106',\n",
       " 'A_107',\n",
       " 'A_108',\n",
       " 'A_117',\n",
       " 'A_118',\n",
       " 'A_119',\n",
       " 'A_120',\n",
       " 'A_126',\n",
       " 'A_137',\n",
       " 'A_173',\n",
       " 'A_180',\n",
       " 'A_183',\n",
       " 'A_184',\n",
       " 'A_188',\n",
       " 'A_211',\n",
       " 'A_212',\n",
       " 'A_213',\n",
       " 'A_215',\n",
       " 'A_216',\n",
       " 'A_220',\n",
       " 'A_221',\n",
       " 'A_223',\n",
       " 'A_224',\n",
       " 'A_227',\n",
       " 'A_230',\n",
       " 'A_241',\n",
       " 'A_271',\n",
       " 'A_299',\n",
       " 'A_301',\n",
       " 'A_302',\n",
       " 'A_303',\n",
       " 'A_304',\n",
       " 'A_305',\n",
       " 'A_306',\n",
       " 'A_308',\n",
       " 'A_309',\n",
       " 'A_310',\n",
       " 'A_311',\n",
       " 'A_312',\n",
       " 'A_313',\n",
       " 'A_315',\n",
       " 'A_364',\n",
       " 'A_369',\n",
       " 'A_370',\n",
       " 'A_371',\n",
       " 'A_372',\n",
       " 'A_373',\n",
       " 'A_374',\n",
       " 'A_375',\n",
       " 'A_398',\n",
       " 'A_433',\n",
       " 'A_434',\n",
       " 'A_435',\n",
       " 'A_436',\n",
       " 'A_437',\n",
       " 'A_440',\n",
       " 'A_441',\n",
       " 'A_442',\n",
       " 'A_443',\n",
       " 'A_444',\n",
       " 'A_447',\n",
       " 'A_448',\n",
       " 'A_451',\n",
       " 'A_481',\n",
       " 'A_482',\n",
       " 'A_483',\n",
       " 'A_484',\n",
       " 'A_485',\n",
       " 'A_488',\n",
       " 'A_50',\n",
       " 'A_57',\n",
       " 'A_76',\n",
       " 'A_78',\n",
       " 'A_79',\n",
       " 'A_94'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binding_residues = set()\n",
    "for binding_pred_file in prediction_files:\n",
    "    df = pd.read_csv(os.path.join(predicted_binding_sites_folder, binding_pred_file))\n",
    "    #print(df.iloc[0,9].strip())\n",
    "    #print(type(df.iloc[0,9].strip()))\n",
    "    for token in df.iloc[0,9].strip().split(' '):\n",
    "        binding_residues.add(token)\n",
    "binding_residues #binding site residues which are predicted, an unique set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = set()\n",
    "for binding_pred_file in prediction_files:\n",
    "    df = pd.read_csv(os.path.join(predicted_binding_sites_folder, binding_pred_file))\n",
    "    #print(df.iloc[0,9].strip())\n",
    "    #print(type(df.iloc[0,9].strip()))\n",
    "    lst = []\n",
    "    #print(df.iloc[0,9].strip().split(' '))\n",
    "    \n",
    "    for token in df.iloc[0,9].strip().split(' '):\n",
    "        lst.append(token)\n",
    "        \n",
    "    s.add(''.join(lst))\n",
    "    \n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(binding_residues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid(vectors):\n",
    "    \"\"\"\n",
    "        vectors: a list of lists for eg: [[1,2,3], [4,5,6], [7,8,9]]\n",
    "        note that each inner list is a list of float values\n",
    "        and a inner list is a row vector in R^3 for eg: given, [1,2,3], 1 is the x-coordinate, 2 is the y-coordinate and 3 is the z-coordinate\n",
    "        \n",
    "        returns a  centroid as a row_vector in R^3\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    #we are going to represent vectors as a 2d matrix, with each row being a row vector in R^3\n",
    "    two_d_matrix = np.array(vectors)\n",
    "    n = len(vectors)\n",
    "    centroid = 1/n * np.sum(two_d_matrix, axis=0)\n",
    "    return centroid\n",
    "\n",
    "def get_residue_ids_from_p2rank_predictions(pred_csv):\n",
    "    \"\"\"\n",
    "        pred_csv (str): abs file path to a prediction.csv generated by p2rank\n",
    "        return a sorted list of residues_id, for eg ['A_1', 'A_2'...]\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    pred_result_df = pd.read_csv(pred_csv)\n",
    "    #going to get the top result, a.k.a first row\n",
    "    \n",
    "    return sorted(pred_result_df.iloc[0, 9].strip().split(' '), key=lambda token: int(token.split('_')[-1]))\n",
    "\n",
    "def get_mutant_code_from_pred_csv(pred_csv):\n",
    "    \n",
    "    \"\"\"\n",
    "        pred_csv (str): for this function, don't need to get abs file path.\n",
    "        for eg: CYP3A4.3.pdbqt_predictions.csv\n",
    "        \n",
    "        return just the mutant code, for eg: CYP3A4.3\n",
    "    \"\"\"\n",
    "    return pred_csv.split('_')[0].replace('.pdbqt', '')\n",
    "\n",
    "def get_pdb_with_mutant_code(folder, mutant_code):\n",
    "    \"\"\"\n",
    "        folder (str): path to folder which stores pdbs generated by scwrl\n",
    "        eg: 'pdb_f/scwrl_out'\n",
    "        mutant_code (str): mutant\n",
    "    \"\"\"\n",
    "    \n",
    "    mutant_pdbs = os.listdir(folder)\n",
    "    \n",
    "    return list(filter(lambda file: mutant_code in file, mutant_pdbs))[0]\n",
    "\n",
    "def get_vectors_of_residue_r_group(residue_ids, mutant_code, pdb_folder):\n",
    "    \"\"\"\n",
    "        residue_ids (list of str): for eg: ['A_50','A_57','A_76','A_78'], it must be sorted\n",
    "        mutant_code (str): for eg: CYP3A4.11\n",
    "        pdb_folder (str): for eg: 'pdb_f/scwrl_out'\n",
    "        \n",
    "        return map for eg 'A_50': [vector_of_atom1_res_50, vector_of_atom2_res50] etc\n",
    "\n",
    "    \"\"\"\n",
    "    import os\n",
    "    #get correct pdb\n",
    "    pdb_files = os.listdir(pdb_folder)\n",
    "    mutant_pdb = list(filter(lambda file: mutant_code in file, pdb_files))[0]\n",
    "    \n",
    "    import re\n",
    "    pattern = re.compile('^ATOM')\n",
    "    back_bone_atoms = set(['N', 'CA', 'O', 'C'])\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    with open(pdb_folder + '/' + mutant_pdb, 'r') as mutant_pdb:\n",
    "        lines = mutant_pdb.readlines()\n",
    "        \n",
    "        for res_id in residue_ids:\n",
    "            chain, res_num = res_id.split('_')\n",
    "            result[res_id] = []\n",
    "            \n",
    "            for line in lines:\n",
    "                if pattern.match(line):\n",
    "                    tokens = re.split(\"\\s{1,}\", line)\n",
    "                    \n",
    "                    from_pdb_chain_type = tokens[4]\n",
    "                    from_pdb_res_number = tokens[5]\n",
    "                    from_pdb_atom = tokens[2]\n",
    "                    \n",
    "                    if chain == from_pdb_chain_type and res_num == from_pdb_res_number and not from_pdb_atom in back_bone_atoms:\n",
    "                        #print(from_pdb_atom)\n",
    "                        x_coord = float(tokens[6])\n",
    "                        y_coord = float(tokens[7])\n",
    "                        z_coord = float(tokens[8])\n",
    "                        \n",
    "                        result[res_id].append([x_coord, y_coord, z_coord])\n",
    "                                    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_vectors_of_all_atoms_of_residues(mutant_code, pdb_folder):\n",
    "    \"\"\"\n",
    "        mutant_code (str): for eg: CYP3A4.11\n",
    "        pdb_folder (str): for eg: 'pdb_f/scwrl_out'\n",
    "        \n",
    "        return map for eg '50': [vector_of_atom1_res_50, vector_of_atom2_res50] etc\n",
    "\n",
    "    \"\"\"\n",
    "    import os\n",
    "    #get correct pdb\n",
    "    pdb_files = os.listdir(pdb_folder)\n",
    "    mutant_pdb = list(filter(lambda file: mutant_code in file, pdb_files))[0]\n",
    "    \n",
    "    import re\n",
    "    pattern = re.compile('^ATOM')\n",
    "    \n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    with open(pdb_folder + '/' + mutant_pdb, 'r') as mutant_pdb:\n",
    "        lines = mutant_pdb.readlines()\n",
    "        \n",
    "            \n",
    "        for line in lines:\n",
    "            if pattern.match(line):\n",
    "                tokens = re.split(\"\\s{1,}\", line)\n",
    "                    \n",
    "                from_pdb_res_number = tokens[5]\n",
    "                    \n",
    "                x_coord = float(tokens[6])\n",
    "                y_coord = float(tokens[7])\n",
    "                z_coord = float(tokens[8]) \n",
    "                \n",
    "                if from_pdb_res_number not in result:\n",
    "                    result[from_pdb_res_number]= [[x_coord, y_coord, z_coord]]\n",
    "                    continue\n",
    "                    \n",
    "                result[from_pdb_res_number].append([x_coord, y_coord, z_coord])\n",
    "                          \n",
    "    return result\n",
    "\n",
    "def get_id_to_resname_map(pdb_file):\n",
    "    import re\n",
    "    \n",
    "    id_to_res_name_map = {}\n",
    "    \n",
    "    pattern = re.compile('^ATOM')\n",
    "    \n",
    "    with open(pdb_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            \n",
    "            if pattern.match(line):\n",
    "                tokens = re.split(\"\\s{1,}\", line)\n",
    "                res_num = tokens[5]\n",
    "                \n",
    "                if int(res_num) not in id_to_res_name_map:\n",
    "                    id_to_res_name_map[int(res_num)] = tokens[3]\n",
    "            \n",
    "    return id_to_res_name_map\n",
    "\n",
    "def get_id_names_map_for_binding_res(res_ids, id_to_res_name_map):\n",
    "    #res_ids example: A_50', 'A_57', 'A_76', 'A_78', must be sorted\n",
    "    #id_to_res_name_map: {50: 'HIS', 57: 'HIS'}. i.e residue number : 3 residue letter\n",
    "    \n",
    "    id_to_res_name_for_binding_sites = {}\n",
    "    \n",
    "    for res_id in res_ids:\n",
    "        _, res_num = res_id.split('_')\n",
    "        \n",
    "        if int(res_num) in id_to_res_name_map:\n",
    "            id_to_res_name_for_binding_sites[int(res_num)] = id_to_res_name_map[int(res_num)]\n",
    "    \n",
    "    return id_to_res_name_for_binding_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "\n",
    "1/3 * np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_centroid([[1,2,3], [4,5,6], [7,8,9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence that res id for each mutant may not correspond to the same amino acid residue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 'ILE'),\n",
       " (57, 'PHE'),\n",
       " (76, 'ASP'),\n",
       " (78, 'GLN'),\n",
       " (79, 'GLN'),\n",
       " (94, 'LEU'),\n",
       " (105, 'ARG'),\n",
       " (106, 'ARG'),\n",
       " (107, 'PRO'),\n",
       " (107, 'SER'),\n",
       " (108, 'PHE'),\n",
       " (117, 'ALA'),\n",
       " (118, 'ILE'),\n",
       " (119, 'ALA'),\n",
       " (119, 'CYS'),\n",
       " (119, 'LEU'),\n",
       " (119, 'PHE'),\n",
       " (119, 'SER'),\n",
       " (119, 'THR'),\n",
       " (119, 'TRP'),\n",
       " (119, 'VAL'),\n",
       " (120, 'ILE'),\n",
       " (120, 'LEU'),\n",
       " (120, 'PHE'),\n",
       " (120, 'TRP'),\n",
       " (126, 'TRP'),\n",
       " (137, 'PHE'),\n",
       " (173, 'LYS'),\n",
       " (180, 'SER'),\n",
       " (183, 'VAL'),\n",
       " (184, 'ILE'),\n",
       " (188, 'SER'),\n",
       " (211, 'LEU'),\n",
       " (212, 'ALA'),\n",
       " (212, 'ARG'),\n",
       " (213, 'PHE'),\n",
       " (215, 'PHE'),\n",
       " (216, 'LEU'),\n",
       " (220, 'PHE'),\n",
       " (221, 'LEU'),\n",
       " (223, 'ILE'),\n",
       " (224, 'THR'),\n",
       " (227, 'PRO'),\n",
       " (230, 'ILE'),\n",
       " (241, 'PHE'),\n",
       " (271, 'PHE'),\n",
       " (299, 'SER'),\n",
       " (301, 'ALA'),\n",
       " (301, 'ILE'),\n",
       " (301, 'PHE'),\n",
       " (301, 'TRP'),\n",
       " (302, 'PHE'),\n",
       " (303, 'ILE'),\n",
       " (304, 'ALA'),\n",
       " (304, 'PHE'),\n",
       " (304, 'TRP'),\n",
       " (305, 'ALA'),\n",
       " (305, 'GLY'),\n",
       " (305, 'PHE'),\n",
       " (305, 'SER'),\n",
       " (305, 'VAL'),\n",
       " (306, 'GLY'),\n",
       " (308, 'GLU'),\n",
       " (309, 'ALA'),\n",
       " (309, 'CYS'),\n",
       " (309, 'PHE'),\n",
       " (309, 'THR'),\n",
       " (309, 'VAL'),\n",
       " (310, 'THR'),\n",
       " (311, 'SER'),\n",
       " (312, 'CYS'),\n",
       " (312, 'SER'),\n",
       " (313, 'VAL'),\n",
       " (315, 'SER'),\n",
       " (364, 'LEU'),\n",
       " (369, 'ILE'),\n",
       " (369, 'PHE'),\n",
       " (369, 'VAL'),\n",
       " (370, 'ALA'),\n",
       " (370, 'CYS'),\n",
       " (370, 'PHE'),\n",
       " (370, 'VAL'),\n",
       " (371, 'ILE'),\n",
       " (371, 'MET'),\n",
       " (372, 'ARG'),\n",
       " (373, 'ALA'),\n",
       " (373, 'HIS'),\n",
       " (373, 'LEU'),\n",
       " (373, 'PHE'),\n",
       " (373, 'VAL'),\n",
       " (374, 'GLN'),\n",
       " (374, 'GLU'),\n",
       " (375, 'ARG'),\n",
       " (398, 'SER'),\n",
       " (433, 'THR'),\n",
       " (434, 'PRO'),\n",
       " (435, 'PHE'),\n",
       " (436, 'GLY'),\n",
       " (437, 'SER'),\n",
       " (440, 'ARG'),\n",
       " (441, 'ASN'),\n",
       " (442, 'CYS'),\n",
       " (443, 'ILE'),\n",
       " (444, 'GLY'),\n",
       " (447, 'PHE'),\n",
       " (448, 'ALA'),\n",
       " (451, 'ASN'),\n",
       " (481, 'GLY'),\n",
       " (482, 'LEU'),\n",
       " (483, 'LEU'),\n",
       " (484, 'GLN'),\n",
       " (485, 'PRO'),\n",
       " (488, 'PRO')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_map = {} # (res_num, res_name)\n",
    "\n",
    "for file in os.listdir('pdb_f/scwrl_out'):\n",
    "    \n",
    "    hm = get_id_to_resname_map(f'pdb_f/scwrl_out/{file}')\n",
    "    mt_code = file.replace('.pdb', \"\")\n",
    "    res_ids = get_residue_ids_from_p2rank_predictions(f'predicted_binding_sites/{mt_code}.pdbqt_predictions.csv')\n",
    "    binding_hm = get_id_names_map_for_binding_res(res_ids, hm)\n",
    "    \n",
    "    for res_num in binding_hm:\n",
    "        res_name = hm[res_num]\n",
    "        \n",
    "        if (res_num, res_name) not in hash_map:\n",
    "            hash_map[(res_num, res_name)] = 1\n",
    "            \n",
    "        else:\n",
    "            hash_map[(res_num, res_name)] += 1\n",
    "\n",
    "sorted(hash_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 2d contact matrix for binding residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "A_481\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(68, 68)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(62, 62)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(59, 59)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(60, 60)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(60, 60)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(63, 63)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(62, 62)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_305\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(60, 60)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(68, 68)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(60, 60)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(60, 60)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(60, 60)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(60, 60)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(58, 58)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(59, 59)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(61, 61)\n",
      "A_306\n",
      "A_436\n",
      "A_444\n",
      "(59, 59)\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('predicted_binding_sites'):\n",
    "    \n",
    "    if '_predictions' not in file:\n",
    "        continue\n",
    "    \n",
    "    mutant_code = get_mutant_code_from_pred_csv(file)\n",
    "    res_ids = get_residue_ids_from_p2rank_predictions(f'predicted_binding_sites/{file}')\n",
    "    residue_vectors_map = get_vectors_of_residue_r_group(res_ids, mutant_code, 'pdb_f/scwrl_out')\n",
    "\n",
    "    residue_id_centroid_map = {}\n",
    "\n",
    "    for residue_id in residue_vectors_map:\n",
    "        list_of_vectors = residue_vectors_map[residue_id]\n",
    "    \n",
    "        if len(list_of_vectors) == 0:\n",
    "            print(residue_id)\n",
    "            continue\n",
    "    \n",
    "        residue_id_centroid_map[residue_id] = compute_centroid(list_of_vectors)\n",
    "\n",
    "    num_residues = len(residue_id_centroid_map)\n",
    "    dim = (num_residues, num_residues)\n",
    "    two_d_matrix_1 = np.zeros(shape=dim)\n",
    "    print(two_d_matrix_1.shape)\n",
    "\n",
    "    processed_pairs = set()\n",
    "    i = 0\n",
    "    for res_i in residue_id_centroid_map:\n",
    "        j= 0\n",
    "    \n",
    "        for res_j in residue_id_centroid_map:\n",
    "            if res_j == res_i:\n",
    "                j += 1\n",
    "                continue\n",
    "        \n",
    "            if (res_i, res_j) in processed_pairs or (res_j, res_i) in processed_pairs:\n",
    "                j+= 1\n",
    "                continue\n",
    "            d_i = residue_id_centroid_map[res_i]\n",
    "            d_j = residue_id_centroid_map[res_j]\n",
    "            euclidean_dist = np.linalg.norm(d_i - d_j)\n",
    "        \n",
    "            #because 2d map is square and symmetric\n",
    "            two_d_matrix_1[i, j] = euclidean_dist\n",
    "            two_d_matrix_1[j, i] = euclidean_dist\n",
    "        \n",
    "            processed_pairs.add((res_i, res_j))\n",
    "            processed_pairs.add((res_j, res_i))\n",
    "        \n",
    "            j+= 1\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    np.save(f'2DContactMaps/CYP3A4_Mutants/{mutant_code}', two_d_matrix_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 2d contact maps for entire protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "for file in os.listdir('pdb_f/scwrl_out'):\n",
    "        \n",
    "    mutant_code = file.replace(\".pdb\", \"\")\n",
    "    residue_vectors_map = get_vectors_of_all_atoms_of_residues(mutant_code, 'pdb_f/scwrl_out')\n",
    "    residue_id_centroid_map = {}\n",
    "\n",
    "    for residue_id in residue_vectors_map:\n",
    "        list_of_vectors = residue_vectors_map[residue_id]\n",
    "        \n",
    "        if len(list_of_vectors) == 0:\n",
    "            print(residue_id)\n",
    "            continue\n",
    "    \n",
    "        residue_id_centroid_map[residue_id] = compute_centroid(list_of_vectors)\n",
    "\n",
    "    num_residues = len(residue_id_centroid_map)\n",
    "    dim = (num_residues, num_residues)\n",
    "    two_d_matrix = np.zeros(shape=dim)\n",
    "\n",
    "    processed_pairs = set()\n",
    "    i = 0\n",
    "    for res_i in residue_id_centroid_map:\n",
    "        j= 0\n",
    "    \n",
    "        for res_j in residue_id_centroid_map:\n",
    "            if res_j == res_i:\n",
    "                j += 1\n",
    "                continue\n",
    "        \n",
    "            if (res_i, res_j) in processed_pairs or (res_j, res_i) in processed_pairs:\n",
    "                j+= 1\n",
    "                continue\n",
    "            d_i = residue_id_centroid_map[res_i]\n",
    "            d_j = residue_id_centroid_map[res_j]\n",
    "            euclidean_dist = np.linalg.norm(d_i - d_j)\n",
    "        \n",
    "            #because 2d map is square and symmetric\n",
    "            two_d_matrix[i, j] = euclidean_dist\n",
    "            two_d_matrix[j, i] = euclidean_dist\n",
    "        \n",
    "            processed_pairs.add((res_i, res_j))\n",
    "            processed_pairs.add((res_j, res_i))\n",
    "        \n",
    "            j+= 1\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    np.save(f'2DContactMaps/CYP3A4_Mutants_Entire/{mutant_code}', two_d_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d contact for Entire vs 2d contact for Binding Site Residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: kaleido in /home/howc/anaconda3/lib/python3.8/site-packages (0.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "entire_2d_files = os.listdir('2DContactMaps/CYP3A4_Mutants_Entire')\n",
    "binding_res_2d_files = os.listdir('2DContactMaps/CYP3A4_Mutants_Binding')\n",
    "\n",
    "for i in range(49):\n",
    "    mt_code = entire_2d_files[i].replace('.npy', \"\")\n",
    "    \n",
    "    first_entire = np.load('2DContactMaps/CYP3A4_Mutants_Entire/' + entire_2d_files[i])\n",
    "    first_binding = np.load('2DContactMaps/CYP3A4_Mutants_Binding/' + binding_res_2d_files[i])\n",
    "    \n",
    "    entire_name = f'{mt_code} entire protein 2D map'\n",
    "    binding_name = f'{mt_code} binding residue protein 2D map'\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[entire_name, binding_name])\n",
    "\n",
    "    entire_data = go.Heatmap(\n",
    "            z=first_entire,\n",
    "        )\n",
    "\n",
    "    fig.add_trace(\n",
    "        entire_data,\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    binding_data = go.Heatmap(\n",
    "            z=first_binding,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        binding_data,\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.update_layout(height=500, width=700)\n",
    "    fig.write_image(f\"2DContactMaps/plots/{mt_code}.jpeg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "entire_2d_files = os.listdir('2DContactMaps/CYP3A4_Mutants_Entire')\n",
    "binding_res_2d_files = os.listdir('2DContactMaps/CYP3A4_Mutants_Binding')\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Entire protein vs Binding residues 2D contact maps'])\n",
    "\n",
    "for i in range(49):\n",
    "    mt_code = entire_2d_files[i].replace('.npy', \"\")\n",
    "    \n",
    "    first_entire = np.load('2DContactMaps/CYP3A4_Mutants_Entire/' + entire_2d_files[i])\n",
    "    first_binding = np.load('2DContactMaps/CYP3A4_Mutants_Binding/' + binding_res_2d_files[i])\n",
    "    \n",
    "    entire_name = f'{mt_code} entire'\n",
    "    binding_name = f'{mt_code} binding'\n",
    "    \n",
    "    entire_data = go.Heatmap(\n",
    "            z=first_entire,\n",
    "            name=entire_name,\n",
    "            visible = 'legendonly'\n",
    "        )\n",
    "\n",
    "    fig.add_trace(\n",
    "        entire_data,\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    binding_data = go.Heatmap(\n",
    "            z=first_binding,\n",
    "            name=binding_name,\n",
    "            visible = 'legendonly'\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        binding_data,\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    \n",
    "steps = []\n",
    "\n",
    "prev = 0\n",
    "\n",
    "for i in range(0, 49, 2):\n",
    "    \n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', ['legendonly'] * len(fig.data)],\n",
    "    )\n",
    "\n",
    "    step['args'][1][i] = True\n",
    "    step['args'][1][i + 1] = True\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    steps = steps,\n",
    ")]\n",
    "\n",
    "fig.layout.sliders = sliders\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CYP3A4-S119F.npy',\n",
       " 'CYP3A4-A370F.npy',\n",
       " 'CYP3A4-T433S.npy',\n",
       " 'CYP3A4-F304W.npy',\n",
       " 'CYP3A4-F304A.npy',\n",
       " 'CYP3A4-M371I.npy',\n",
       " 'CYP3A4.3.npy',\n",
       " 'CYP3A4-I120F.npy',\n",
       " 'CYP3A4-I120L.npy',\n",
       " 'CYP3A4-I369V.npy',\n",
       " 'CYP3A4-T309F.npy',\n",
       " 'CYP3A4-A305G.npy',\n",
       " 'CYP3A4-T363M.npy',\n",
       " 'CYP3A4-C98A.npy',\n",
       " 'CYP3A4-L373H.npy',\n",
       " 'CYP3A4-T309A.npy',\n",
       " 'CYP3A4-A370V.npy',\n",
       " 'CYP3A4-T309V.npy',\n",
       " 'CYP3A4-N104D.npy',\n",
       " 'CYP3A4-I120W.npy',\n",
       " 'CYP3A4-I301A.npy',\n",
       " 'CYP3A4-S119C.npy',\n",
       " 'CYP3A4-P107S.npy',\n",
       " 'CYP3A4-L373A.npy',\n",
       " 'CYP3A4-A305S.npy',\n",
       " 'CYP3A4-I301W.npy',\n",
       " 'CYP3A4-S119T.npy',\n",
       " 'CYP3A4-I301F.npy',\n",
       " 'CYP3A4-A305V.npy',\n",
       " 'CYP3A4-T103A.npy',\n",
       " 'CYP3A4-L373F.npy',\n",
       " 'CYP3A4-A370C.npy',\n",
       " 'CYP3A4-C98S.npy',\n",
       " 'CYP3A4.11.npy',\n",
       " 'CYP3A4.12.npy',\n",
       " 'CYP3A4-V376T.npy',\n",
       " 'CYP3A4-S119V.npy',\n",
       " 'CYP3A4-I369F.npy',\n",
       " 'CYP3A4-A305F.npy',\n",
       " 'CYP3A4-S119W.npy',\n",
       " 'CYP3A4-S119A.npy',\n",
       " 'CYP3A4-S312C.npy',\n",
       " 'CYP3A4-T309C.npy',\n",
       " 'CYP3A4-S119L.npy',\n",
       " 'CYP3A4-R212A.npy',\n",
       " 'CYP3A4-C98F.npy',\n",
       " 'CYP3A4-L373V.npy',\n",
       " 'CYP3A4-E374Q.npy',\n",
       " 'CYP3A4-C98W.npy']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(steps)\n",
    "binding_res_2d_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.tools import make_subplots\n",
    "\n",
    "fig = make_subplots(1, 2)\n",
    "\n",
    "fig.add_scatter(y=[1, 3, 2], row=1, col=1, visible=True)\n",
    "fig.add_scatter(y=[3, 1, 1.5], row=1, col=1, visible='legendonly')\n",
    "fig.add_scatter(y=[2, 2, 1], row=1, col=1, visible='legendonly')\n",
    "fig.add_scatter(y=[1, 3, 2], row=1, col=2, visible=True)\n",
    "fig.add_scatter(y=[1.5, 2, 2.5], row=1, col=2, visible='legendonly')\n",
    "fig.add_scatter(y=[2.5, 1.2, 2.9], row=1, col=2, visible='legendonly')\n",
    "\n",
    "print(len(fig.data))\n",
    "\n",
    "steps = []\n",
    "for i in range(3):\n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', ['legendonly'] * len(fig.data)],\n",
    "    )\n",
    "    #step['args'][1][i] = True\n",
    "    #step['args'][1][i+3] = True\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    steps = steps,\n",
    ")]\n",
    "\n",
    "fig.layout.sliders = sliders\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50+57+76+78+79+94+105+106+107+108+118+119+120+126+137+180+183+184+188+212+213+215+216+220+221+223+224+227+230+241+271+299+301+302+303+304+305+306+309+310+312+313+364+369+370+371+372+373+374+375+434+435+436+437+440+441+442+443+444+447+448+451+482+483'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ids = get_residue_ids_from_p2rank_predictions('predicted_binding_sites/CYP3A4-S119T.pdbqt_predictions.csv')\n",
    "residue_ids = []\n",
    "\n",
    "for res_id in res_ids:\n",
    "    residue_ids.append(res_id.split('_')[-1])\n",
    "\n",
    "s1 = '+'.join(residue_ids)\n",
    "s1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
