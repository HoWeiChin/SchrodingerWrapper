{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_df = pd.read_csv('full.csv')\n",
    "full_df.iloc[0, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "complex_data_columns = list(full_df.columns)\n",
    "complex_codes = full_df['complex_code'].unique()\n",
    "data = []\n",
    "\n",
    "for complex_code in complex_codes:\n",
    "    x_train = full_df[full_df['complex_code'] == complex_code].iloc[:, 0:-1].to_numpy()\n",
    "    x_train_complex = np.mean(x_train, axis=0).tolist() ##Important: complex vector is generated by averaging atom vectors\n",
    "    x_train_complex.append(complex_code)\n",
    "    \n",
    "    data.append(x_train_complex)\n",
    "    \n",
    "\n",
    "full_data = pd.DataFrame(data, columns=complex_data_columns)\n",
    "full_data.to_csv('from_scratch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set: 60\n",
      "size of validation set: 31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(full_data, test_size=0.33, random_state=42)\n",
    "\n",
    "train_df.to_csv('train_df.csv')\n",
    "val_df.to_csv('val_df.csv')\n",
    "\n",
    "print(f'size of training set: {train_df.shape[0]}')\n",
    "print(f'size of validation set: {val_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns available: ['element_type', 'residue_type', 'hydrophobic_interactions', 'hydrogen_bonds', 'water_bridges', 'pi_stacks', 'pi_cation_interactions', 'halogen_bonds', 'salt_bridges', 'metal_complexes', 'origin_type', 'complex_code']\n",
      "No. of columns present: 11\n"
     ]
    }
   ],
   "source": [
    "#check the types of columns available\n",
    "print(f'columns available: {list(train_df.columns)}')\n",
    "print(f'No. of columns present: {len(list(train_df.columns)[:-1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable distribution for training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(complex_codes, db_path):\n",
    "    db = pd.read_csv(db_path)\n",
    "    targets = []\n",
    "    for code in complex_codes:\n",
    "        mt_code, cid, _ = code.split('_')\n",
    "        y_train = db.loc[(db['mt_code'] == mt_code) & (db['cid'] == int(cid))].iloc[0, 11] \n",
    "        targets.append(y_train)\n",
    "    \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complex_codes = train_df.loc[:, 'complex_code'].unique()\n",
    "val_complex_codes = val_df.loc[:, 'complex_code'].unique()\n",
    "\n",
    "\n",
    "train_targets = get_targets(train_complex_codes, 'test_db/single_near_mutant_CYP3A4_db.csv')\n",
    "val_targets = get_targets(val_complex_codes, 'test_db/single_near_mutant_CYP3A4_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAF1CAYAAAAwfzllAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAh9klEQVR4nO3de7xVdZ3/8ddHIDHRUEIjUHEaf3kXEIwy/Y2Zl8qAmbTwUjg50oxTP50m0y6WjXNxKhvHR9PPH6YNM6FkkUrZo9E0xhwtBwsNRUMmDJQESRxIcUA/vz/WOrQ9nMPZcM7hy96+no/Heey11/Xz3d/DfrMuZ63ITCRJ0va1U+kCJEl6JTKAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwABW0yJiaUS8vZtpx0TEo9u7ph1JVL4WEc9ExH39vK2rI+KSJubbNyLWRcSA/qyniTrOjoi7C2z3QxFx5fbebm9FxM4R8UhE7FW6FvUfA1h9IjN/lJlv7Gm+iLg0Ir6+PWoq4K3ACcCozDyqr1baVXhl5p9m5mU9LZuZv8rMIZn5Yr2ueRHxJ31V244sIl4FfBr4Qv1+dERkRAwsVE/T28/MF4DrgIv6vzKVYgCrbZT6Ym2wH7A0M39buA5VJgOPZOYTfbGyAr9f1wPTImLn7bxdbScGsLbWmIh4MCKejYhvRMRggIj4g4hY3jFTRFwUEU9ExNqIeDQijo+Ik4FPAu+rD4s+UM/7+oiYGxG/iYjHIuLchvXsEhEz68O6iyLi4522s7Te1oPAbyNiYERcHBFL6m0/HBF/2DD/2RHxHxHxDxGxJiL+KyLeUo9fFhErI2Jad43vrtaIOAf4KvDmum2f62LZN0TEnRGxOiKejohZETG0Yfo+EfHtiFhVz/PliDgIuLphvWvqef85Iv66Hl4UEac0rGdgvf5xjXtdEfE3wDHAl+t1fTki/ikiruhU53ci4oIu6r86Ir7YadwtEfHRerjbz73TMpvtCXbeM4+ID9bteiYi/i0i9qvHR913K+vfwQcj4tAuOwveAfx7w/u76tc1dfvf3ESfdPX79YGIeLxe5pJoODUTETs1fA6rI+LGiNhzC9v//Yj497otT0fENzq2nZnLgWeAid20T60uM/3xp6kfYClwH/B6YE9gEfCn9bQ/AJbXw28ElgGvr9+PBt5QD18KfL3Tev8d+AowGBgDrAKOr6ddXk/fAxgFPNixnYaaFgD7ALvU406ra9wJeB/wW2BEPe1sYCPwx8AA4K+BXwH/BOwMnAisBYZ08xlsqdazgbu38Pn9PtUh6p2B4VRfyFfW0wYADwD/AOxar/+t3a0X+Gfgr+vhzwCzGqa9i2rPr+OzT2Bg/X4e8CcN8x4FPAnsVL9/LfAcsHcX9R9b92vU7/cAnm/o554+97u7qqlzXcAU4DHgIGAg1WHke+ppJwH3A0OBqOcZ0c3n/Z/AaQ3vu9put33S1e8XcDCwjup0w6uALwIbgLfX818A/Jjqd3Vn4P8BN2xh+zcAn6o/s0193jB9LvB/Sv/b96d/ftwD1ta6KjOfzMzfAN+hCqHOXqT68jk4IgZl5tLMXNLVyiJiH6ovs4syc31mLqDak3x/Pct7gb/NzGey2iO4qpualmXm8wCZ+c26xpcy8xvAYqqg6fDLzPxaVudFv0H15fpXmflCZt4G/A/VF/PW1rpFmflYZt5eb2cV8CXgf9eTj6IKrwsz87f1+pu9aOl6YFJEvLp+f0Y9rpma7gOeBY6vR00F5mXmU13M/iOqADmmfn8qcG9mPlmvq6fPvVkfAv4uMxdl5kbgb6mOvOxHFXa7AQdS/UdgUWau6GY9Q6n+M9WtHvqkQ+Pv16nAdzLz7sz8H6r//DTeUP9DwKcyc3lW53EvBU6N7g9fb6A6dfH6bvp8bd0OtSEDWFvr1w3DzwFDOs+QmY9R7QlcCqyMiNkR8fpu1vd64DeZ2fhF+TgwsmH6soZpjcNdjqsPES6oDzGvAQ6l2rPr0BguHaHdedxm7Wqi1i2KiL3qz+KJiPhv4OsNde0DPF4HzlapP+9FwLvrEJ5EkwFcmwmcVQ+fBfxrN9tJYDZwej3qDGBWx/QmPvdm7Qf8Y8N6fkO1tzsyM+8Evkx1xOKpiJgREbt3s55nqMK6Wz30SYfG36+X/T5m5nPA6k6139RQ+yKq/5Du3U0JH6/bdl9EPBQRH+w0fTdgzZbaoNZlAKtfZOb1mflWqi+kBP6+Y1KnWZ8E9oyIxi/KfYGOC2dWUB3O67BPV5vrGKj3kq4BPgwMy8yhwEKqL7ne6qnWnvxdXevhmbk7Vdh11LUM2LebPaVmHll2A1UwTgYerkO5K12t6+vA5Ig4guqQ7s09bOfU+nN+EzAHtvpz77hI7dUN417XMLwM+FBmDm342SUz7wHIzKsy80jgEOB/ARd2U+uD9fQOXbV9S33S1XIv+32MiF2AYZ1qf0en2gdndSHYZtvPzF9n5rmZ+XqqveevRETj0ZeDqE5NqA0ZwOpzEfHGiHhbVFdvrqfao3yxnvwUMDoidgLIzGXAPcDfRcTgiDgcOIff7VndCHwiIvaIiJFUX/BbsivVF92qupY/ptoT67Umau3JblTnD9fUbWkMjvuovtwvj4hd6/UfXU97ChgV1Z/VdGc21fnrP2PLe79PAb/XqV3Lqc6X/iswp+NQflcy82dUn+1XgX/LzDX1pKY/9/pQ7xPAWRExoN7re0PDLFdT9fkh9bpeExGn1cMTIuJNETGIKsjX87vfrc6+x8sPJ68CXurU/i31SVe+RXWk4S11f3yOlwf21cDfNFw0NjwiJne3/Yg4LSI6Av0Zqs+w40/GRlJda/HjHmpSizKA1R92prp46mmqQ9Z7UV39DPDN+nV1RPy0Hj6d6gKVJ4GbgM9m5u31tL8ClgO/BH5A9QX4QncbzsyHgSuAe6nC5jDgP/qiUU3U2pPPAeOozrneCny7Y0J9PvrdVOeef0XV5vfVk+8EHgJ+HRFPd7Xi+jzovcBbqM5rd+cfqfZgn4mIxvPpM6k+qy4PP3dyA/B2GoJ+Gz73c6nCbjXVnuw9Deu6ieqIyez6sPBCqiuaAXan2tN+hurw/2qqC6G68h3gwI7TH/Xh4r8B/qM+RDyRLfRJVzLzIeAjVP/hWUF1jnYlv/ud/EeqC6dui4i1VOH5pi1sfwLwk4hYVy93fmb+sl7XGcDM+lyy2lDH1YxSS4iIPwOmZmbnC2XUCxFxLNWh6NGZ+VLpevpKREwHDs7MC/pp/UOoztEe0BCcfbHenakOPR+bmSv7ar3asZS+cYG0RRExguqQ3b3AAcBfUl2Eoz5SH849H/hqO4UvQGbO6Ot1RsS7gTuoDj1/Efg51Z8r9Zl6r/fAvlyndjwegtaO7lVUf0u5lupQ7C1Uf4erPhDVjT7WACOAK4sW0zomU52CeJLqP4VT00OJ2gYegpYkqQD3gCVJKsAAliSpgO16EdZrX/vaHD169PbcpCRJxdx///1PZ+bwrqZt1wAePXo08+fP356blCSpmIh4vLtpHoKWJKkAA1iSpAIMYEmSCvBOWJLU4jZs2MDy5ctZv3596VJesQYPHsyoUaMYNGhQ08sYwJLU4pYvX85uu+3G6NGjieiLJ29qa2Qmq1evZvny5ey///5NL+chaElqcevXr2fYsGGGbyERwbBhw7b6CIQBLEltwPAta1s+fwNYktQra9as4Stf2bZnpLzzne9kzZo1Tc9/88038/DDD2/TtrZGb9rULM8BS1KbGX3xrX26vqWXv2uL0zvC6rzzztts2osvvsiAAQO6XfZ73/veVtVy8803c8opp3DwwQc3vczGjRsZOHDr4m5Lbeor7gFLknrl4osvZsmSJYwZM4YLL7yQefPmcdxxx3HGGWdw2GGHATBlyhSOPPJIDjnkEGbM+N1jmkePHs3TTz/N0qVLOeiggzj33HM55JBDOPHEE3n++edftp177rmHuXPncuGFFzJmzBiWLFnCNddcw4QJEzjiiCN4z3vew3PPPQfA2WefzUc/+lGOO+44LrroIpYsWcLEiROZMGECn/nMZxgyZMim9X7hC19gwoQJHH744Xz2s5/tsk0rVqzg2GOPZcyYMRx66KH86Ec/6vXn5h6wJKlXLr/8chYuXMiCBQsAmDdvHvfddx8LFy7cdFXwddddx5577snzzz/PhAkTeM973sOwYcNetp7Fixdzww03cM011/De976XOXPmcNZZZ22a/pa3vIVJkyZxyimncOqppwIwdOhQzj33XAA+/elPc+211/KRj3wEgF/84hf84Ac/YMCAAZxyyimcf/75nH766Vx99dWb1nnbbbexePFi7rvvPjKTSZMmcdddd23WpiuuuIKTTjqJT33qU7z44oubgr43DGBJUp876qijXvYnOVdddRU33XQTAMuWLWPx4sWbBfD+++/PmDFjADjyyCNZunRpj9tZuHAhn/70p1mzZg3r1q3jpJNO2jTttNNO23T4+9577+Xmm28G4IwzzuBjH/sYUAXwbbfdxtixYwFYt24dixcvZt99933ZdiZMmMAHP/hBNmzYwJQpUzbV2RsegpYk9bldd9110/C8efP4wQ9+wL333ssDDzzA2LFju/yTnZ133nnT8IABA9i4cWOP2zn77LP58pe/zM9//nM++9nPvmy9jTV0JzP5xCc+wYIFC1iwYAGPPfYY55xzzmbzHXvssdx1112MHDmS97///fzLv/xLj+vuSUvvAff1hQb9oaeLFySp1e22226sXbu22+nPPvsse+yxB69+9at55JFH+PGPf9xn21q7di0jRoxgw4YNzJo1i5EjR3a53MSJE5kzZw7ve9/7mD179qbxJ510EpdccglnnnkmQ4YM4YknnmDQoEGbbefxxx9n5MiRnHvuufz2t7/lpz/9KR/4wAe2uR3gHrAkqZeGDRvG0UcfzaGHHsqFF1642fSTTz6ZjRs3cvjhh3PJJZcwceLEbd7W1KlT+cIXvsDYsWNZsmQJl112GW9605s44YQTOPDAA7td7sorr+RLX/oSRx11FCtWrOA1r3kNACeeeCJnnHEGb37zmznssMM49dRTWbt27WZtmjdvHmPGjGHs2LHMmTOH888/f5vb0CEys9cradb48eOzL58H7B6wJMGiRYs46KCDSpexQ3vuuefYZZddiAhmz57NDTfcwC233NKn2+iqHyLi/swc39X8LX0IWpKkZtx///18+MMfJjMZOnQo1113XemSDGBJUvs75phjeOCBB0qX8TKeA5YkqQADWJKkAgxgSZIKMIAlSSrAAJYkbXeND0PosD0eAdhhez3WcEu8ClqS2s2lr+nj9T3bt+vrxrY8AjAzyUx22mnr9ie35bGGfc09YElSr1x00UUv23O99NJLueKKK1i3bh3HH38848aN47DDDuvxxhedHwHY3fIdjy4877zzGDduHMuWLeOyyy7jwAMP5IQTTuD000/ni1/8IgBLlizh5JNP5sgjj+SYY47hkUce6fKxhldddRUHH3wwhx9+OFOnTu2/D6uBe8CSpF6ZOnUqF1xwwaY91xtvvJHvf//7DB48mJtuuondd9+dp59+mokTJzJp0iQiosv1dH4E4MaNG7tcHuDRRx/la1/7Gl/5yleYP38+c+bM4Wc/+xkbN25k3LhxHHnkkQBMnz6dq6++mgMOOICf/OQnnHfeedx5552bPdbw8ssv55e//CU777wza9as6d8PrGYAS5J6ZezYsaxcuZInn3ySVatWsccee7DvvvuyYcMGPvnJT3LXXXex00478cQTT/DUU0/xute9rqn1ZmaXywPst99+m+4pfffddzN58mR22WUXAN797ncD1aMF77nnHk477bRN63zhhRe63Nbhhx/OmWeeyZQpU5gyZcq2fhRbxQCWJPXaqaeeyre+9S1+/etfbzqEO2vWLFatWsX999/PoEGDGD16dJePIezOlpZvfNRgd880eOmllxg6dOimPeotufXWW7nrrruYO3cul112GQ899BADB/ZvRHoOWJLUa1OnTmX27Nl861vf2nRY99lnn2WvvfZi0KBB/PCHP+Txxx/f4jo6PwKw2eXf+ta38p3vfIf169ezbt06br21elDP7rvvzv777883v/lNoArqjttRNm7rpZdeYtmyZRx33HF8/vOfZ82aNaxbt653H0gTDGBJUq8dcsghrF27lpEjRzJixAgAzjzzTObPn8/48eOZNWvWFh8XCJs/1rDZ5SdMmMCkSZM44ogj+KM/+iPGjx+/6XGDs2bN4tprr+WII47gkEMO2XQhV+NjDRcvXsxZZ53FYYcdxtixY/mLv/gLhg4d2ncfTjd8HGE/83GEkvqbjyOszvcOGTKE5557jmOPPZYZM2Ywbty47VqDjyOUJL3iTJ8+nYcffpj169czbdq07R6+28IAliS1vOuvv750CVvNc8CSJBVgAEtSG9ie1/Noc9vy+RvAktTiBg8ezOrVqw3hQjKT1atXM3jw4K1aznPAktTiRo0axfLly1m1alXpUl6xBg8ezKhRo7ZqGQNYklrcoEGD2H///UuXoa3kIWhJkgowgCVJKsAAliSpAANYkqQCDGBJkgpo6iroiFgKrAVeBDZm5viI2BP4BjAaWAq8NzOf6Z8yJUlqL1uzB3xcZo5peKrDxcAdmXkAcEf9XpIkNaE3h6AnAzPr4ZnAlF5XI0nSK0SzAZzAbRFxf0RMr8ftnZkrAOrXvbpaMCKmR8T8iJjvXVokSao0eyesozPzyYjYC7g9Ih5pdgOZOQOYATB+/HhvVCpJEk3uAWfmk/XrSuAm4CjgqYgYAVC/ruyvIiVJajc9BnBE7BoRu3UMAycCC4G5wLR6tmnALf1VpCRJ7aaZQ9B7AzdFRMf812fm9yPiP4EbI+Ic4FfAaf1XpiRJ7aXHAM7M/wKO6GL8auD4/ihKkqR2552wJEkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKmApgM4IgZExM8i4rv1+z0j4vaIWFy/7tF/ZUqS1F62Zg/4fGBRw/uLgTsy8wDgjvq9JElqQlMBHBGjgHcBX20YPRmYWQ/PBKb0aWWSJLWxZveArwQ+DrzUMG7vzFwBUL/u1belSZLUvnoM4Ig4BViZmfdvywYiYnpEzI+I+atWrdqWVUiS1Haa2QM+GpgUEUuB2cDbIuLrwFMRMQKgfl3Z1cKZOSMzx2fm+OHDh/dR2ZIktbYeAzgzP5GZozJzNDAVuDMzzwLmAtPq2aYBt/RblZIktZne/B3w5cAJEbEYOKF+L0mSmjBwa2bOzHnAvHp4NXB835ckSVL7805YkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgE9BnBEDI6I+yLigYh4KCI+V4/fMyJuj4jF9ese/V+uJEntoZk94BeAt2XmEcAY4OSImAhcDNyRmQcAd9TvJUlSE3oM4Kysq98Oqn8SmAzMrMfPBKb0R4GSJLWjps4BR8SAiFgArARuz8yfAHtn5gqA+nWvbpadHhHzI2L+qlWr+qhsSZJaW1MBnJkvZuYYYBRwVEQc2uwGMnNGZo7PzPHDhw/fxjIlSWovW3UVdGauAeYBJwNPRcQIgPp1ZV8XJ0lSu2rmKujhETG0Ht4FeDvwCDAXmFbPNg24pZ9qlCSp7QxsYp4RwMyIGEAV2Ddm5ncj4l7gxog4B/gVcFo/1ilJUlvpMYAz80FgbBfjVwPH90dRkiS1O++EJUlSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkF9BjAEbFPRPwwIhZFxEMRcX49fs+IuD0iFteve/R/uZIktYdm9oA3An+ZmQcBE4E/j4iDgYuBOzLzAOCO+r0kSWpCjwGcmSsy86f18FpgETASmAzMrGebCUzppxolSWo7W3UOOCJGA2OBnwB7Z+YKqEIa2KvPq5MkqU01HcARMQSYA1yQmf+9FctNj4j5ETF/1apV21KjJEltp6kAjohBVOE7KzO/XY9+KiJG1NNHACu7WjYzZ2Tm+MwcP3z48L6oWZKkltfMVdABXAssyswvNUyaC0yrh6cBt/R9eZIktaeBTcxzNPB+4OcRsaAe90ngcuDGiDgH+BVwWr9UKElSG+oxgDPzbiC6mXx835YjSdIrg3fCkiSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCDGBJkgowgCVJKsAAliSpAANYkqQCBpYuoLeWDj5js3Gj119foBJJkprnHrAkSQUYwJIkFWAAS5JUgAEsSVIBBrAkSQUYwJIkFWAAS5JUgAEsSVIBBrAkSQUYwJIkFWAAS5JUgAEsSVIBPQZwRFwXESsjYmHDuD0j4vaIWFy/7tG/ZUqS1F6a2QP+Z+DkTuMuBu7IzAOAO+r3kiSpST0GcGbeBfym0+jJwMx6eCYwpW/LkiSpvW3rOeC9M3MFQP26V3czRsT0iJgfEfNXrVq1jZuTJKm99PtFWJk5IzPHZ+b44cOH9/fmJElqCdsawE9FxAiA+nVl35UkSVL729YAngtMq4enAbf0TTmSJL0yNPNnSDcA9wJvjIjlEXEOcDlwQkQsBk6o30uSpCYN7GmGzDy9m0nH93EtkiS9YngnLEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKkAA1iSpAIMYEmSCjCAJUkqwACWJKmAHu8F3YqWDj6jy/Gj11+/nSuRJKlr7gFLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgFteSvKvtDV7Sy9laUkqa+4ByxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklSAASxJUgEGsCRJBRjAkiQVYABLklTAwNIFbE9LB5+x2bjR668vUIkk6ZXOPWBJkgowgCVJKsAAliSpAANYkqQCXlEXYXWlqwuz+modo9dfz+iLb93m7XmBWGXp5e8qXULL6/x7uCOyn9ufv4cv5x6wJEkF9CqAI+LkiHg0Ih6LiIv7qihJktrdNgdwRAwA/gl4B3AwcHpEHNxXhUmS1M56swd8FPBYZv5XZv4PMBuY3DdlSZLU3noTwCOBZQ3vl9fjJElSDyIzt23BiNOAkzLzT+r37weOysyPdJpvOjC9fvtG4NFtL3czrwWe7sP1ldQubWmXdoBt2RG1SzugfdrSLu2A/mnLfpk5vKsJvfkzpOXAPg3vRwFPdp4pM2cAM3qxnW5FxPzMHN8f697e2qUt7dIOsC07onZpB7RPW9qlHbD929KbQ9D/CRwQEftHxKuAqcDcvilLkqT2ts17wJm5MSI+DPwbMAC4LjMf6rPKJElqY726E1Zmfg/4Xh/Vsi365dB2Ie3SlnZpB9iWHVG7tAPapy3t0g7Yzm3Z5ouwJEnStvNWlJIkFdCyAdzKt8GMiKUR8fOIWBAR8+txe0bE7RGxuH7do3SdXYmI6yJiZUQsbBjXbe0R8Ym6jx6NiJPKVN21btpyaUQ8UffNgoh4Z8O0HbItEbFPRPwwIhZFxEMRcX49vqX6ZQvtaMU+GRwR90XEA3VbPlePb6k+gS22peX6Baq7OEbEzyLiu/X7cn2SmS33Q3XR1xLg94BXAQ8AB5euayvqXwq8ttO4zwMX18MXA39fus5uaj8WGAcs7Kl2qluUPgDsDOxf99mA0m3ooS2XAh/rYt4dti3ACGBcPbwb8Iu63pbqly20oxX7JIAh9fAg4CfAxFbrkx7a0nL9Utf3UeB64Lv1+2J90qp7wO14G8zJwMx6eCYwpVwp3cvMu4DfdBrdXe2TgdmZ+UJm/hJ4jKrvdgjdtKU7O2xbMnNFZv60Hl4LLKK6K11L9csW2tGdHbIdAFlZV78dVP8kLdYnsMW2dGeHbUtEjALeBXy1YXSxPmnVAG7122AmcFtE3F/fKQxg78xcAdUXEbBXseq2Xne1t2o/fTgiHqwPUXccjmqJtkTEaGAs1V5Ky/ZLp3ZAC/ZJfahzAbASuD0zW7ZPumkLtF6/XAl8HHipYVyxPmnVAI4uxrXS5dxHZ+Y4qidJ/XlEHFu6oH7Siv30f4E3AGOAFcAV9fgdvi0RMQSYA1yQmf+9pVm7GLfDtKWLdrRkn2Tmi5k5huougUdFxKFbmL0V29JS/RIRpwArM/P+ZhfpYlyftqNVA7ip22DuqDLzyfp1JXAT1WGNpyJiBED9urJchVutu9pbrp8y86n6y+Yl4Bp+d8hph25LRAyiCq1ZmfntenTL9UtX7WjVPumQmWuAecDJtGCfNGpsSwv2y9HApIhYSnXa8m0R8XUK9kmrBnDL3gYzInaNiN06hoETgYVU9U+rZ5sG3FKmwm3SXe1zgakRsXNE7A8cANxXoL6mdfxDrP0hVd/ADtyWiAjgWmBRZn6pYVJL9Ut37WjRPhkeEUPr4V2AtwOP0GJ9At23pdX6JTM/kZmjMnM0VWbcmZlnUbJPSl6N1psf4J1UV0kuAT5Vup6tqPv3qK6sewB4qKN2YBhwB7C4ft2zdK3d1H8D1eGmDVT/QzxnS7UDn6r76FHgHaXrb6It/wr8HHiw/gc4YkdvC/BWqkNjDwIL6p93tlq/bKEdrdgnhwM/q2teCHymHt9SfdJDW1quXxrq+wN+dxV0sT7xTliSJBXQqoegJUlqaQawJEkFGMCSJBVgAEuSVIABLElSAQawJEkFGMCSJBVgAEuSVMD/B1xi2guLWR0AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(train_targets, label='train targets')\n",
    "plt.hist(val_targets, label='val targets')\n",
    "plt.title(\"histogram of activity values (targets)\") \n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested\n",
    "class BindingSiteDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_data, path_to_db):\n",
    "        self.df = pd.read_csv(path_to_data)\n",
    "        self.db = pd.read_csv(path_to_db)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        complex_name = self.df.iloc[index, -1]\n",
    "        mt_code, cid, _ = complex_name.split('_')\n",
    "        #get activity value for the first reaction in our dataframe\n",
    "        y_true = self.db.loc[(self.db['mt_code'] == mt_code) & (self.db['cid'] == int(cid))].iloc[0, 11] \n",
    "        return torch.from_numpy(self.df.iloc[index, :-1].to_numpy().astype('float64')).float(), torch.Tensor([y_true]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cuda to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleNNRegression, self).__init__()\n",
    "        \n",
    "        self.fnn = torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_features=12, out_features=20),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(in_features=20, out_features=5),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(in_features=5, out_features=1),\n",
    "                torch.nn.ReLU()  \n",
    "        )\n",
    "    \n",
    "    def forward(self, complex_vector):\n",
    "        return self.fnn(complex_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    model.train()\n",
    "    error = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # convert from torch to numpy again, X,y are of type torch.tensor, so that we could perform normalisation\n",
    "        X_np = X.detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        \n",
    "        #convert from numpy to torch again\n",
    "        X_normalised, y_normalised = torch.from_numpy(scaler.fit_transform(X_np)), torch.from_numpy(scaler.fit_transform(y_np))\n",
    "        \n",
    "        pred = model(X_normalised)\n",
    "        loss = loss_fn(pred, y_normalised)\n",
    "        error += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return error\n",
    "\n",
    "def test_loop(val_dataloader, model, loss_fn):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    model.eval()\n",
    "    error = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in val_dataloader:\n",
    "            # convert from torch to numpy again, X,y are of type torch.tensor, so that we could perform normalisation\n",
    "            X_np = X.detach().cpu().numpy()\n",
    "            y_np = y.detach().cpu().numpy()\n",
    "            \n",
    "            #convert from numpy to torch again\n",
    "            X_normalised, y_normalised = torch.from_numpy(scaler.fit_transform(X_np)), torch.from_numpy(scaler.fit_transform(y_np))\n",
    "            pred = model(X_normalised)\n",
    "            error += loss_fn(pred, y_normalised).item()\n",
    "            \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train loss: 1.025572419166565 val loss: 0.9808834862196818\n",
      "epoch 2 train loss: 0.9605900645256042 val loss: 0.9663691418536473\n",
      "epoch 3 train loss: 0.9075292348861694 val loss: 0.9386023331899196\n",
      "epoch 4 train loss: 0.8453579545021057 val loss: 0.9734110787976533\n",
      "epoch 5 train loss: 0.7994402050971985 val loss: 1.0327176266109745\n",
      "epoch 6 train loss: 0.7298619747161865 val loss: 1.0892740488052368\n",
      "epoch 7 train loss: 0.6719976663589478 val loss: 1.1218332052230835\n",
      "epoch 8 train loss: 0.6032238602638245 val loss: 1.2342036962509155\n",
      "epoch 9 train loss: 0.5235078930854797 val loss: 1.3329273462295532\n",
      "epoch 10 train loss: 0.4342758357524872 val loss: 1.45780348777771\n",
      "epoch 11 train loss: 0.3674229383468628 val loss: 1.7768745422363281\n",
      "epoch 12 train loss: 0.3227258622646332 val loss: 1.7299941778182983\n",
      "epoch 13 train loss: 0.2656433880329132 val loss: 0.988638699054718\n",
      "epoch 14 train loss: 0.22569800913333893 val loss: 2.389899492263794\n",
      "epoch 15 train loss: 0.22710399329662323 val loss: 2.3341784477233887\n",
      "epoch 16 train loss: 0.26859936118125916 val loss: 2.460401773452759\n",
      "epoch 17 train loss: 0.2235904484987259 val loss: 3.8187670707702637\n",
      "epoch 18 train loss: 0.4426179528236389 val loss: 2.5390524864196777\n",
      "epoch 19 train loss: 0.24686740338802338 val loss: 1.8759273290634155\n",
      "epoch 20 train loss: 0.3270663619041443 val loss: 1.7873826026916504\n",
      "epoch 21 train loss: 0.38242825865745544 val loss: 1.7452841997146606\n",
      "epoch 22 train loss: 0.34694379568099976 val loss: 1.8338731527328491\n",
      "epoch 23 train loss: 0.30376142263412476 val loss: 2.059387683868408\n",
      "epoch 24 train loss: 0.24115684628486633 val loss: 2.6750099658966064\n",
      "epoch 25 train loss: 0.2879357635974884 val loss: 2.5855414867401123\n",
      "epoch 26 train loss: 0.27791982889175415 val loss: 2.2229559421539307\n",
      "epoch 27 train loss: 0.21485254168510437 val loss: 1.656934142112732\n",
      "epoch 28 train loss: 0.25914695858955383 val loss: 1.0000001192092896\n",
      "epoch 29 train loss: 0.23276814818382263 val loss: 1.8040035963058472\n",
      "epoch 30 train loss: 0.19721803069114685 val loss: 1.7832627296447754\n",
      "epoch 31 train loss: 0.23410944640636444 val loss: 1.792961597442627\n",
      "epoch 32 train loss: 0.21534988284111023 val loss: 1.5336430072784424\n",
      "epoch 33 train loss: 0.2132817804813385 val loss: 1.6250147819519043\n",
      "epoch 34 train loss: 0.22625726461410522 val loss: 1.5443880558013916\n",
      "epoch 35 train loss: 0.20513100922107697 val loss: 1.6415971517562866\n",
      "epoch 36 train loss: 0.20871393382549286 val loss: 1.7140724658966064\n",
      "epoch 37 train loss: 0.21486541628837585 val loss: 1.8052325248718262\n",
      "epoch 38 train loss: 0.19858665764331818 val loss: 1.7710379362106323\n",
      "epoch 39 train loss: 0.2083558738231659 val loss: 1.711632251739502\n",
      "epoch 40 train loss: 0.20570321381092072 val loss: 1.7171584367752075\n",
      "epoch 41 train loss: 0.19603905081748962 val loss: 2.0237157344818115\n",
      "epoch 42 train loss: 0.2052728831768036 val loss: 2.1273107528686523\n",
      "epoch 43 train loss: 0.19701194763183594 val loss: 1.9057936668395996\n",
      "epoch 44 train loss: 0.20086345076560974 val loss: 1.8800472021102905\n",
      "epoch 45 train loss: 0.20384830236434937 val loss: 2.2173149585723877\n",
      "epoch 46 train loss: 0.1967899203300476 val loss: 2.170870065689087\n",
      "epoch 47 train loss: 0.2015603482723236 val loss: 1.9306645393371582\n",
      "epoch 48 train loss: 0.197956383228302 val loss: 2.019374132156372\n",
      "epoch 49 train loss: 0.19677291810512543 val loss: 2.321207284927368\n",
      "epoch 50 train loss: 0.19907288253307343 val loss: 2.377638339996338\n",
      "epoch 51 train loss: 0.1950163096189499 val loss: 2.148954153060913\n",
      "epoch 52 train loss: 0.19616040587425232 val loss: 2.2723336219787598\n",
      "epoch 53 train loss: 0.19617138803005219 val loss: 2.1107988357543945\n",
      "epoch 54 train loss: 0.19373339414596558 val loss: 1.832100510597229\n",
      "epoch 55 train loss: 0.1955510526895523 val loss: 2.1083765029907227\n",
      "epoch 56 train loss: 0.19457288086414337 val loss: 2.0510733127593994\n",
      "epoch 57 train loss: 0.19392167031764984 val loss: 1.970897912979126\n",
      "epoch 58 train loss: 0.19524674117565155 val loss: 2.1608262062072754\n",
      "epoch 59 train loss: 0.19378580152988434 val loss: 2.3347599506378174\n",
      "epoch 60 train loss: 0.19386792182922363 val loss: 1.9250062704086304\n",
      "epoch 61 train loss: 0.1939334273338318 val loss: 2.3145575523376465\n",
      "epoch 62 train loss: 0.19252632558345795 val loss: 2.350781202316284\n",
      "epoch 63 train loss: 0.19308318197727203 val loss: 2.383946418762207\n",
      "epoch 64 train loss: 0.19232051074504852 val loss: 2.2147176265716553\n",
      "epoch 65 train loss: 0.19206023216247559 val loss: 2.4071857929229736\n",
      "epoch 66 train loss: 0.1923845410346985 val loss: 2.1117873191833496\n",
      "epoch 67 train loss: 0.19152624905109406 val loss: 2.2678492069244385\n",
      "epoch 68 train loss: 0.19188497960567474 val loss: 2.207699775695801\n",
      "epoch 69 train loss: 0.19119451940059662 val loss: 2.3385326862335205\n",
      "epoch 70 train loss: 0.19104234874248505 val loss: 2.3450238704681396\n",
      "epoch 71 train loss: 0.19095946848392487 val loss: 2.283923864364624\n",
      "epoch 72 train loss: 0.1904587745666504 val loss: 2.5150325298309326\n",
      "epoch 73 train loss: 0.19055284559726715 val loss: 1.072444200515747\n",
      "epoch 74 train loss: 0.19008898735046387 val loss: 2.3021442890167236\n",
      "epoch 75 train loss: 0.18995393812656403 val loss: 2.3663926124572754\n",
      "epoch 76 train loss: 0.18978659808635712 val loss: 2.3199167251586914\n",
      "epoch 77 train loss: 0.19038213789463043 val loss: 2.364717960357666\n",
      "epoch 78 train loss: 0.18955855071544647 val loss: 2.2775943279266357\n",
      "epoch 79 train loss: 0.1897955983877182 val loss: 2.3458402156829834\n",
      "epoch 80 train loss: 0.1900019496679306 val loss: 2.3201987743377686\n",
      "epoch 81 train loss: 0.1901930719614029 val loss: 2.5654191970825195\n",
      "epoch 82 train loss: 0.19014224410057068 val loss: 2.4496424198150635\n",
      "epoch 83 train loss: 0.1900169849395752 val loss: 2.580112934112549\n",
      "epoch 84 train loss: 0.1897096484899521 val loss: 2.3404037952423096\n",
      "epoch 85 train loss: 0.18939612805843353 val loss: 2.685842275619507\n",
      "epoch 86 train loss: 0.1897500604391098 val loss: 2.5185370445251465\n",
      "epoch 87 train loss: 0.19001087546348572 val loss: 2.728484630584717\n",
      "epoch 88 train loss: 0.18979451060295105 val loss: 1.0752662420272827\n",
      "epoch 89 train loss: 0.19010597467422485 val loss: 2.3296234607696533\n",
      "epoch 90 train loss: 0.1900954395532608 val loss: 2.730268955230713\n",
      "epoch 91 train loss: 0.18978552520275116 val loss: 1.0635253190994263\n",
      "epoch 92 train loss: 0.18987694382667542 val loss: 2.1524593830108643\n",
      "epoch 93 train loss: 0.18931636214256287 val loss: 2.557882070541382\n",
      "epoch 94 train loss: 0.1894957423210144 val loss: 2.516022205352783\n",
      "epoch 95 train loss: 0.18949632346630096 val loss: 2.3588109016418457\n",
      "epoch 96 train loss: 0.19087858498096466 val loss: 2.4126946926116943\n",
      "epoch 97 train loss: 0.18978723883628845 val loss: 2.5050747394561768\n",
      "epoch 98 train loss: 0.19083259999752045 val loss: 2.3714828491210938\n",
      "epoch 99 train loss: 0.19050492346286774 val loss: 2.7181735038757324\n",
      "epoch 100 train loss: 0.1905166506767273 val loss: 2.1691768169403076\n",
      "epoch 101 train loss: 0.19063887000083923 val loss: 2.551234006881714\n",
      "epoch 102 train loss: 0.18991102278232574 val loss: 2.449291706085205\n",
      "epoch 103 train loss: 0.19031523168087006 val loss: 2.260296106338501\n",
      "epoch 104 train loss: 0.18939949572086334 val loss: 2.729430675506592\n",
      "epoch 105 train loss: 0.18993642926216125 val loss: 2.4869680404663086\n",
      "epoch 106 train loss: 0.189200296998024 val loss: 2.327409505844116\n",
      "epoch 107 train loss: 0.1898607462644577 val loss: 2.613527774810791\n",
      "epoch 108 train loss: 0.18945735692977905 val loss: 2.7646067142486572\n",
      "epoch 109 train loss: 0.19000142812728882 val loss: 2.4174489974975586\n",
      "epoch 110 train loss: 0.18978732824325562 val loss: 2.4497745037078857\n",
      "epoch 111 train loss: 0.1905486136674881 val loss: 2.4439404010772705\n",
      "epoch 112 train loss: 0.19008152186870575 val loss: 2.342536449432373\n",
      "epoch 113 train loss: 0.19051605463027954 val loss: 2.3408188819885254\n",
      "epoch 114 train loss: 0.18977075815200806 val loss: 2.477903127670288\n",
      "epoch 115 train loss: 0.18990060687065125 val loss: 2.317457675933838\n",
      "epoch 116 train loss: 0.18920691311359406 val loss: 2.2710161209106445\n",
      "epoch 117 train loss: 0.18930155038833618 val loss: 2.6636557579040527\n",
      "epoch 118 train loss: 0.18889717757701874 val loss: 2.279564380645752\n",
      "epoch 119 train loss: 0.1890270709991455 val loss: 2.3744704723358154\n",
      "epoch 120 train loss: 0.19123698770999908 val loss: 2.464735269546509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 121 train loss: 0.19012098014354706 val loss: 2.241429567337036\n",
      "epoch 122 train loss: 0.18905414640903473 val loss: 2.604830265045166\n",
      "epoch 123 train loss: 0.1904124915599823 val loss: 2.4985389709472656\n",
      "epoch 124 train loss: 0.189854234457016 val loss: 2.448377847671509\n",
      "epoch 125 train loss: 0.19066879153251648 val loss: 2.7480854988098145\n",
      "epoch 126 train loss: 0.18991619348526 val loss: 2.4778482913970947\n",
      "epoch 127 train loss: 0.1900382936000824 val loss: 2.4200308322906494\n",
      "epoch 128 train loss: 0.18918952345848083 val loss: 2.4150161743164062\n",
      "epoch 129 train loss: 0.18911592662334442 val loss: 2.137371778488159\n",
      "epoch 130 train loss: 0.18859705328941345 val loss: 2.4931716918945312\n",
      "epoch 131 train loss: 0.18867792189121246 val loss: 2.213623285293579\n",
      "epoch 132 train loss: 0.1912679672241211 val loss: 2.4554688930511475\n",
      "epoch 133 train loss: 0.19167263805866241 val loss: 2.249356269836426\n",
      "epoch 134 train loss: 0.1914619654417038 val loss: 2.634355068206787\n",
      "epoch 135 train loss: 0.1912563145160675 val loss: 2.243049383163452\n",
      "epoch 136 train loss: 0.19095033407211304 val loss: 2.542681932449341\n",
      "epoch 137 train loss: 0.18813832104206085 val loss: 2.242025136947632\n",
      "epoch 138 train loss: 0.1914614737033844 val loss: 2.2759652137756348\n",
      "epoch 139 train loss: 0.1909162402153015 val loss: 2.5123226642608643\n",
      "epoch 140 train loss: 0.19412942230701447 val loss: 2.4041945934295654\n",
      "epoch 141 train loss: 0.1929575502872467 val loss: 2.7875702381134033\n",
      "epoch 142 train loss: 0.19329245388507843 val loss: 2.9056923389434814\n",
      "epoch 143 train loss: 0.19062849879264832 val loss: 2.6161646842956543\n",
      "epoch 144 train loss: 0.19046074151992798 val loss: 2.366621732711792\n",
      "epoch 145 train loss: 0.18856197595596313 val loss: 2.256309986114502\n",
      "epoch 146 train loss: 0.18928973376750946 val loss: 2.4742496013641357\n",
      "epoch 147 train loss: 0.1888069361448288 val loss: 2.3267767429351807\n",
      "epoch 148 train loss: 0.1929275393486023 val loss: 2.4032845497131348\n",
      "epoch 149 train loss: 0.19303742051124573 val loss: 2.6194074153900146\n",
      "epoch 150 train loss: 0.193363219499588 val loss: 2.2865843772888184\n",
      "epoch 151 train loss: 0.19248341023921967 val loss: 2.2681689262390137\n",
      "epoch 152 train loss: 0.19200462102890015 val loss: 2.3182101249694824\n",
      "epoch 153 train loss: 0.18801555037498474 val loss: 2.2586686611175537\n",
      "epoch 154 train loss: 0.19169725477695465 val loss: 2.6262667179107666\n",
      "epoch 155 train loss: 0.19047269225120544 val loss: 2.3390448093414307\n",
      "epoch 156 train loss: 0.19527751207351685 val loss: 2.558908224105835\n",
      "epoch 157 train loss: 0.19400151073932648 val loss: 2.494093656539917\n",
      "epoch 158 train loss: 0.19487042725086212 val loss: 2.3554160594940186\n",
      "epoch 159 train loss: 0.19183270633220673 val loss: 1.1268200874328613\n",
      "epoch 160 train loss: 0.19153809547424316 val loss: 2.3060505390167236\n",
      "epoch 161 train loss: 0.18870370090007782 val loss: 2.421241044998169\n",
      "epoch 162 train loss: 0.18946513533592224 val loss: 2.1589443683624268\n",
      "epoch 163 train loss: 0.18866799771785736 val loss: 2.336101531982422\n",
      "epoch 164 train loss: 0.19204610586166382 val loss: 2.2615103721618652\n",
      "epoch 165 train loss: 0.19007763266563416 val loss: 2.0261216163635254\n",
      "epoch 166 train loss: 0.18933658301830292 val loss: 2.3839616775512695\n",
      "epoch 167 train loss: 0.18942363560199738 val loss: 2.618283271789551\n",
      "epoch 168 train loss: 0.18906906247138977 val loss: 2.648253917694092\n",
      "epoch 169 train loss: 0.18969710171222687 val loss: 2.1511659622192383\n",
      "epoch 170 train loss: 0.1893581748008728 val loss: 2.2812728881835938\n",
      "epoch 171 train loss: 0.18994930386543274 val loss: 2.5465245246887207\n",
      "epoch 172 train loss: 0.18913094699382782 val loss: 2.2060303688049316\n",
      "epoch 173 train loss: 0.18943315744400024 val loss: 2.2801709175109863\n",
      "epoch 174 train loss: 0.18836680054664612 val loss: 2.308133602142334\n",
      "epoch 175 train loss: 0.18868286907672882 val loss: 2.2887227535247803\n",
      "epoch 176 train loss: 0.18781791627407074 val loss: 2.330705165863037\n",
      "epoch 177 train loss: 0.18827636539936066 val loss: 2.545206308364868\n",
      "epoch 178 train loss: 0.18762847781181335 val loss: 2.459164619445801\n",
      "epoch 179 train loss: 0.18798598647117615 val loss: 2.3350725173950195\n",
      "epoch 180 train loss: 0.18784098327159882 val loss: 2.2175493240356445\n",
      "epoch 181 train loss: 0.19056794047355652 val loss: 2.3416459560394287\n",
      "epoch 182 train loss: 0.18818332254886627 val loss: 2.4893147945404053\n",
      "epoch 183 train loss: 0.19137164950370789 val loss: 2.2688589096069336\n",
      "epoch 184 train loss: 0.1905364692211151 val loss: 2.1837217807769775\n",
      "epoch 185 train loss: 0.1907953917980194 val loss: 2.6035563945770264\n",
      "epoch 186 train loss: 0.18919841945171356 val loss: 2.164247751235962\n",
      "epoch 187 train loss: 0.18857724964618683 val loss: 2.292841911315918\n",
      "epoch 188 train loss: 0.18766771256923676 val loss: 2.111083507537842\n",
      "epoch 189 train loss: 0.18707402050495148 val loss: 2.369696855545044\n",
      "epoch 190 train loss: 0.18792621791362762 val loss: 2.211507558822632\n",
      "epoch 191 train loss: 0.1877831369638443 val loss: 2.490638017654419\n",
      "epoch 192 train loss: 0.18717119097709656 val loss: 2.276686668395996\n",
      "epoch 193 train loss: 0.18821072578430176 val loss: 2.415412664413452\n",
      "epoch 194 train loss: 0.18743936717510223 val loss: 2.2515578269958496\n",
      "epoch 195 train loss: 0.18844445049762726 val loss: 2.4995152950286865\n",
      "epoch 196 train loss: 0.1874137967824936 val loss: 2.5705697536468506\n",
      "epoch 197 train loss: 0.18784098327159882 val loss: 2.2557528018951416\n",
      "epoch 198 train loss: 0.18698468804359436 val loss: 2.4385228157043457\n",
      "epoch 199 train loss: 0.18702490627765656 val loss: 2.188098669052124\n",
      "epoch 200 train loss: 0.18676070868968964 val loss: 2.440488338470459\n",
      "epoch 201 train loss: 0.18658269941806793 val loss: 2.397989511489868\n",
      "epoch 202 train loss: 0.1866898089647293 val loss: 2.2090904712677\n",
      "epoch 203 train loss: 0.18618908524513245 val loss: 2.4477274417877197\n",
      "epoch 204 train loss: 0.18815483152866364 val loss: 2.3510005474090576\n",
      "epoch 205 train loss: 0.18817344307899475 val loss: 2.129685640335083\n",
      "epoch 206 train loss: 0.18771976232528687 val loss: 2.388021230697632\n",
      "epoch 207 train loss: 0.1915394812822342 val loss: 2.1811535358428955\n",
      "epoch 208 train loss: 0.18961037695407867 val loss: 2.1771128177642822\n",
      "epoch 209 train loss: 0.19010843336582184 val loss: 2.262698173522949\n",
      "epoch 210 train loss: 0.1874946653842926 val loss: 2.8237204551696777\n",
      "epoch 211 train loss: 0.18773581087589264 val loss: 2.442699909210205\n",
      "epoch 212 train loss: 0.18574675917625427 val loss: 2.204015016555786\n",
      "epoch 213 train loss: 0.18659737706184387 val loss: 2.210329294204712\n",
      "epoch 214 train loss: 0.19215640425682068 val loss: 2.3703689575195312\n",
      "epoch 215 train loss: 0.19279637932777405 val loss: 2.581150770187378\n",
      "epoch 216 train loss: 0.19294805824756622 val loss: 2.2526373863220215\n",
      "epoch 217 train loss: 0.19238699972629547 val loss: 2.3126935958862305\n",
      "epoch 218 train loss: 0.19166429340839386 val loss: 2.327634334564209\n",
      "epoch 219 train loss: 0.1906735748052597 val loss: 2.2502169609069824\n",
      "epoch 220 train loss: 0.1868470162153244 val loss: 2.2457430362701416\n",
      "epoch 221 train loss: 0.18936234712600708 val loss: 2.677105665206909\n",
      "epoch 222 train loss: 0.19166766107082367 val loss: 2.3504841327667236\n",
      "epoch 223 train loss: 0.19591191411018372 val loss: 2.439011573791504\n",
      "epoch 224 train loss: 0.19373734295368195 val loss: 2.47904634475708\n",
      "epoch 225 train loss: 0.19200946390628815 val loss: 2.432189464569092\n",
      "epoch 226 train loss: 0.18687134981155396 val loss: 2.541834831237793\n",
      "epoch 227 train loss: 0.18731628358364105 val loss: 2.162147045135498\n",
      "epoch 228 train loss: 0.18675576150417328 val loss: 2.206598997116089\n",
      "epoch 229 train loss: 0.19256779551506042 val loss: 2.318108081817627\n",
      "epoch 230 train loss: 0.1860063523054123 val loss: 2.5304980278015137\n",
      "epoch 231 train loss: 0.1900198608636856 val loss: 2.4031946659088135\n",
      "epoch 232 train loss: 0.18844819068908691 val loss: 2.1938772201538086\n",
      "epoch 233 train loss: 0.18929362297058105 val loss: 2.3119466304779053\n",
      "epoch 234 train loss: 0.18804311752319336 val loss: 2.54650616645813\n",
      "epoch 235 train loss: 0.18732020258903503 val loss: 2.2257776260375977\n",
      "epoch 236 train loss: 0.18870247900485992 val loss: 2.21120023727417\n",
      "epoch 237 train loss: 0.18690727651119232 val loss: 2.75996470451355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 238 train loss: 0.18973813951015472 val loss: 2.298313617706299\n",
      "epoch 239 train loss: 0.18520599603652954 val loss: 2.3143417835235596\n",
      "epoch 240 train loss: 0.18873192369937897 val loss: 2.2949721813201904\n",
      "epoch 241 train loss: 0.1850000023841858 val loss: 2.617244005203247\n",
      "epoch 242 train loss: 0.18801532685756683 val loss: 2.5742082595825195\n",
      "epoch 243 train loss: 0.18519236147403717 val loss: 2.126213550567627\n",
      "epoch 244 train loss: 0.1858247071504593 val loss: 2.262024164199829\n",
      "epoch 245 train loss: 0.1858438104391098 val loss: 2.1793081760406494\n",
      "epoch 246 train loss: 0.18387196958065033 val loss: 2.149310350418091\n",
      "epoch 247 train loss: 0.18534761667251587 val loss: 2.3985984325408936\n",
      "epoch 248 train loss: 0.18345995247364044 val loss: 2.491076707839966\n",
      "epoch 249 train loss: 0.18567787110805511 val loss: 2.5271809101104736\n",
      "epoch 250 train loss: 0.18331627547740936 val loss: 2.0523946285247803\n",
      "epoch 251 train loss: 0.1843995600938797 val loss: 2.2564985752105713\n",
      "epoch 252 train loss: 0.1889888197183609 val loss: 2.4917080402374268\n",
      "epoch 253 train loss: 0.1838335245847702 val loss: 2.5524351596832275\n",
      "epoch 254 train loss: 0.18632462620735168 val loss: 2.7181692123413086\n",
      "epoch 255 train loss: 0.18737664818763733 val loss: 2.290438413619995\n",
      "epoch 256 train loss: 0.18654146790504456 val loss: 2.3365681171417236\n",
      "epoch 257 train loss: 0.18442249298095703 val loss: 2.193495750427246\n",
      "epoch 258 train loss: 0.1825074553489685 val loss: 2.063370943069458\n",
      "epoch 259 train loss: 0.18927139043807983 val loss: 1.0788453817367554\n",
      "epoch 260 train loss: 0.18374048173427582 val loss: 2.166414737701416\n",
      "epoch 261 train loss: 0.18274590373039246 val loss: 2.401498317718506\n",
      "epoch 262 train loss: 0.1841958612203598 val loss: 2.7086832523345947\n",
      "epoch 263 train loss: 0.18323543667793274 val loss: 2.6359670162200928\n",
      "epoch 264 train loss: 0.18403545022010803 val loss: 2.180994749069214\n",
      "epoch 265 train loss: 0.18246370553970337 val loss: 2.6981847286224365\n",
      "epoch 266 train loss: 0.18448545038700104 val loss: 2.149317979812622\n",
      "epoch 267 train loss: 0.18292398750782013 val loss: 2.119704246520996\n",
      "epoch 268 train loss: 0.18390583992004395 val loss: 2.1103689670562744\n",
      "epoch 269 train loss: 0.18422113358974457 val loss: 2.2713980674743652\n",
      "epoch 270 train loss: 0.18411137163639069 val loss: 2.4081714153289795\n",
      "epoch 271 train loss: 0.18507936596870422 val loss: 2.808948516845703\n",
      "epoch 272 train loss: 0.18850643932819366 val loss: 2.496220827102661\n",
      "epoch 273 train loss: 0.187815859913826 val loss: 2.2736425399780273\n",
      "epoch 274 train loss: 0.18662385642528534 val loss: 2.085219144821167\n",
      "epoch 275 train loss: 0.18331876397132874 val loss: 2.1625478267669678\n",
      "epoch 276 train loss: 0.1833972930908203 val loss: 2.2344141006469727\n",
      "epoch 277 train loss: 0.18232110142707825 val loss: 2.137035608291626\n",
      "epoch 278 train loss: 0.1847822517156601 val loss: 2.26488995552063\n",
      "epoch 279 train loss: 0.1878097653388977 val loss: 2.5067873001098633\n",
      "epoch 280 train loss: 0.184160515666008 val loss: 2.260972499847412\n",
      "epoch 281 train loss: 0.18318694829940796 val loss: 2.5850491523742676\n",
      "epoch 282 train loss: 0.18797214329242706 val loss: 2.456639051437378\n",
      "epoch 283 train loss: 0.18806518614292145 val loss: 2.1969306468963623\n",
      "epoch 284 train loss: 0.18850736320018768 val loss: 2.0560944080352783\n",
      "epoch 285 train loss: 0.18409499526023865 val loss: 2.4996044635772705\n",
      "epoch 286 train loss: 0.18338125944137573 val loss: 2.0784265995025635\n",
      "epoch 287 train loss: 0.18162797391414642 val loss: 2.052988290786743\n",
      "epoch 288 train loss: 0.1838674545288086 val loss: 2.18107533454895\n",
      "epoch 289 train loss: 0.1846248209476471 val loss: 2.1532809734344482\n",
      "epoch 290 train loss: 0.18405680358409882 val loss: 1.984981656074524\n",
      "epoch 291 train loss: 0.18346573412418365 val loss: 2.1102170944213867\n",
      "epoch 292 train loss: 0.18118739128112793 val loss: 2.346522569656372\n",
      "epoch 293 train loss: 0.1820615977048874 val loss: 2.278663158416748\n",
      "epoch 294 train loss: 0.18072949349880219 val loss: 2.3091986179351807\n",
      "epoch 295 train loss: 0.18265585601329803 val loss: 2.5035297870635986\n",
      "epoch 296 train loss: 0.1813783198595047 val loss: 2.28481388092041\n",
      "epoch 297 train loss: 0.18123990297317505 val loss: 2.3947913646698\n",
      "epoch 298 train loss: 0.1798006147146225 val loss: 2.191298246383667\n",
      "epoch 299 train loss: 0.17894548177719116 val loss: 2.133347272872925\n",
      "epoch 300 train loss: 0.18089064955711365 val loss: 2.1247262954711914\n",
      "epoch 301 train loss: 0.18496745824813843 val loss: 2.2803688049316406\n",
      "epoch 302 train loss: 0.18057234585285187 val loss: 2.4271318912506104\n",
      "epoch 303 train loss: 0.18692973256111145 val loss: 2.477656602859497\n",
      "epoch 304 train loss: 0.18234039843082428 val loss: 2.4748919010162354\n",
      "epoch 305 train loss: 0.18116992712020874 val loss: 2.4290781021118164\n",
      "epoch 306 train loss: 0.18033534288406372 val loss: 2.1372604370117188\n",
      "epoch 307 train loss: 0.17919141054153442 val loss: 2.2311882972717285\n",
      "epoch 308 train loss: 0.19982223212718964 val loss: 2.288006544113159\n",
      "epoch 309 train loss: 0.1798691302537918 val loss: 2.901787519454956\n",
      "epoch 310 train loss: 0.1949126273393631 val loss: 2.096174955368042\n",
      "epoch 311 train loss: 0.2063758671283722 val loss: 2.186793804168701\n",
      "epoch 312 train loss: 0.18227514624595642 val loss: 2.1556811332702637\n",
      "epoch 313 train loss: 0.21124137938022614 val loss: 1.8714447021484375\n",
      "epoch 314 train loss: 0.2029319554567337 val loss: 1.8667621612548828\n",
      "epoch 315 train loss: 0.1915934681892395 val loss: 2.226296901702881\n",
      "epoch 316 train loss: 0.21190711855888367 val loss: 2.2801787853240967\n",
      "epoch 317 train loss: 0.1859806627035141 val loss: 2.0415420532226562\n",
      "epoch 318 train loss: 0.19559837877750397 val loss: 2.3746984004974365\n",
      "epoch 319 train loss: 0.18482208251953125 val loss: 2.4401588439941406\n",
      "epoch 320 train loss: 0.18948954343795776 val loss: 1.8841304779052734\n",
      "epoch 321 train loss: 0.1913021057844162 val loss: 2.18304705619812\n",
      "epoch 322 train loss: 0.18509723246097565 val loss: 2.255558967590332\n",
      "epoch 323 train loss: 0.18957969546318054 val loss: 1.2613104581832886\n",
      "epoch 324 train loss: 0.17917057871818542 val loss: 2.1179840564727783\n",
      "epoch 325 train loss: 0.18780288100242615 val loss: 2.1864216327667236\n",
      "epoch 326 train loss: 0.17739185690879822 val loss: 2.149195909500122\n",
      "epoch 327 train loss: 0.18592432141304016 val loss: 2.213691234588623\n",
      "epoch 328 train loss: 0.1754855513572693 val loss: 2.1389412879943848\n",
      "epoch 329 train loss: 0.18413156270980835 val loss: 2.3993871212005615\n",
      "epoch 330 train loss: 0.1743895560503006 val loss: 2.486654758453369\n",
      "epoch 331 train loss: 0.1821117103099823 val loss: 2.2725493907928467\n",
      "epoch 332 train loss: 0.17393463850021362 val loss: 2.268254518508911\n",
      "epoch 333 train loss: 0.1793961524963379 val loss: 2.263124942779541\n",
      "epoch 334 train loss: 0.17408566176891327 val loss: 2.279496669769287\n",
      "epoch 335 train loss: 0.17769959568977356 val loss: 2.3760061264038086\n",
      "epoch 336 train loss: 0.19499555230140686 val loss: 2.291018009185791\n",
      "epoch 337 train loss: 0.1774582713842392 val loss: 2.767042636871338\n",
      "epoch 338 train loss: 0.1955195963382721 val loss: 2.3089804649353027\n",
      "epoch 339 train loss: 0.17656512558460236 val loss: 1.9774110317230225\n",
      "epoch 340 train loss: 0.18313762545585632 val loss: 2.2082912921905518\n",
      "epoch 341 train loss: 0.18282489478588104 val loss: 2.53330659866333\n",
      "epoch 342 train loss: 0.1717357635498047 val loss: 2.272427558898926\n",
      "epoch 343 train loss: 0.17573155462741852 val loss: 2.455569267272949\n",
      "epoch 344 train loss: 0.17266172170639038 val loss: 2.503239154815674\n",
      "epoch 345 train loss: 0.17502564191818237 val loss: 2.121910810470581\n",
      "epoch 346 train loss: 0.17270421981811523 val loss: 1.9282420873641968\n",
      "epoch 347 train loss: 0.17291401326656342 val loss: 1.9613094329833984\n",
      "epoch 348 train loss: 0.17231658101081848 val loss: 2.3360936641693115\n",
      "epoch 349 train loss: 0.17177757620811462 val loss: 2.252357006072998\n",
      "epoch 350 train loss: 0.1724300980567932 val loss: 2.2814066410064697\n",
      "epoch 351 train loss: 0.17078208923339844 val loss: 2.0889339447021484\n",
      "epoch 352 train loss: 0.1714981347322464 val loss: 2.23525071144104\n",
      "epoch 353 train loss: 0.16942113637924194 val loss: 1.9805407524108887\n",
      "epoch 354 train loss: 0.1702178567647934 val loss: 2.350598096847534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 355 train loss: 0.16898176074028015 val loss: 2.272434949874878\n",
      "epoch 356 train loss: 0.16877633333206177 val loss: 2.212167501449585\n",
      "epoch 357 train loss: 0.16841457784175873 val loss: 1.9760245084762573\n",
      "epoch 358 train loss: 0.1669289469718933 val loss: 2.2824671268463135\n",
      "epoch 359 train loss: 0.17464584112167358 val loss: 2.554138660430908\n",
      "epoch 360 train loss: 0.18937046825885773 val loss: 2.681420087814331\n",
      "epoch 361 train loss: 0.17864087224006653 val loss: 2.6176180839538574\n",
      "epoch 362 train loss: 0.17892217636108398 val loss: 2.51115345954895\n",
      "epoch 363 train loss: 0.18269090354442596 val loss: 2.2734785079956055\n",
      "epoch 364 train loss: 0.16546984016895294 val loss: 2.2723419666290283\n",
      "epoch 365 train loss: 0.1944960206747055 val loss: 2.2602250576019287\n",
      "epoch 366 train loss: 0.1941538155078888 val loss: 2.2116522789001465\n",
      "epoch 367 train loss: 0.20096063613891602 val loss: 2.1577999591827393\n",
      "epoch 368 train loss: 0.2021373063325882 val loss: 2.008485794067383\n",
      "epoch 369 train loss: 0.2012527883052826 val loss: 2.3176589012145996\n",
      "epoch 370 train loss: 0.1982065588235855 val loss: 1.9627912044525146\n",
      "epoch 371 train loss: 0.19603565335273743 val loss: 2.2331886291503906\n",
      "epoch 372 train loss: 0.19373707473278046 val loss: 2.3969714641571045\n",
      "epoch 373 train loss: 0.1950516253709793 val loss: 2.5419881343841553\n",
      "epoch 374 train loss: 0.19504427909851074 val loss: 2.4091110229492188\n",
      "epoch 375 train loss: 0.19218990206718445 val loss: 2.740159273147583\n",
      "epoch 376 train loss: 0.1955229938030243 val loss: 2.9001991748809814\n",
      "epoch 377 train loss: 0.2050708383321762 val loss: 2.9828710556030273\n",
      "epoch 378 train loss: 0.19439244270324707 val loss: 2.6337015628814697\n",
      "epoch 379 train loss: 0.2106517106294632 val loss: 2.4927384853363037\n",
      "epoch 380 train loss: 0.22300682961940765 val loss: 1.2856476306915283\n",
      "epoch 381 train loss: 0.19666799902915955 val loss: 3.1963326930999756\n",
      "epoch 382 train loss: 0.23592831194400787 val loss: 2.200331687927246\n",
      "epoch 383 train loss: 0.2327091544866562 val loss: 3.168426990509033\n",
      "epoch 384 train loss: 0.207540825009346 val loss: 2.9612488746643066\n",
      "epoch 385 train loss: 0.24917621910572052 val loss: 2.6232450008392334\n",
      "epoch 386 train loss: 0.2061806470155716 val loss: 2.5963406562805176\n",
      "epoch 387 train loss: 0.2201720029115677 val loss: 3.07511568069458\n",
      "epoch 388 train loss: 0.20525021851062775 val loss: 3.1845903396606445\n",
      "epoch 389 train loss: 0.19515718519687653 val loss: 2.8551437854766846\n",
      "epoch 390 train loss: 0.2044920027256012 val loss: 2.818709135055542\n",
      "epoch 391 train loss: 0.19413916766643524 val loss: 3.054809093475342\n",
      "epoch 392 train loss: 0.2056788057088852 val loss: 2.7948596477508545\n",
      "epoch 393 train loss: 0.19342820346355438 val loss: 2.5601284503936768\n",
      "epoch 394 train loss: 0.2004280686378479 val loss: 3.033870220184326\n",
      "epoch 395 train loss: 0.19413688778877258 val loss: 3.3517754077911377\n",
      "epoch 396 train loss: 0.19724072515964508 val loss: 2.448928117752075\n",
      "epoch 397 train loss: 0.1948850452899933 val loss: 3.1127448081970215\n",
      "epoch 398 train loss: 0.19465652108192444 val loss: 2.8444411754608154\n",
      "epoch 399 train loss: 0.19537395238876343 val loss: 3.296168327331543\n",
      "epoch 400 train loss: 0.19272691011428833 val loss: 2.7765629291534424\n",
      "epoch 401 train loss: 0.19561640918254852 val loss: 3.199786901473999\n",
      "epoch 402 train loss: 0.19169634580612183 val loss: 3.1272499561309814\n",
      "epoch 403 train loss: 0.19613316655158997 val loss: 2.997831106185913\n",
      "epoch 404 train loss: 0.19174519181251526 val loss: 3.1710729598999023\n",
      "epoch 405 train loss: 0.1941550374031067 val loss: 2.9329755306243896\n",
      "epoch 406 train loss: 0.19230209290981293 val loss: 3.035151481628418\n",
      "epoch 407 train loss: 0.1929321438074112 val loss: 3.2787485122680664\n",
      "epoch 408 train loss: 0.1927613914012909 val loss: 2.872745990753174\n",
      "epoch 409 train loss: 0.19201895594596863 val loss: 3.1492726802825928\n",
      "epoch 410 train loss: 0.19293545186519623 val loss: 2.9448204040527344\n",
      "epoch 411 train loss: 0.19148263335227966 val loss: 2.841982126235962\n",
      "epoch 412 train loss: 0.19285233318805695 val loss: 2.6944429874420166\n",
      "epoch 413 train loss: 0.19129803776741028 val loss: 2.555140733718872\n",
      "epoch 414 train loss: 0.1926269829273224 val loss: 3.280773162841797\n",
      "epoch 415 train loss: 0.19131793081760406 val loss: 3.053225040435791\n",
      "epoch 416 train loss: 0.19231627881526947 val loss: 3.3798019886016846\n",
      "epoch 417 train loss: 0.19142606854438782 val loss: 2.8383166790008545\n",
      "epoch 418 train loss: 0.19200235605239868 val loss: 3.5867040157318115\n",
      "epoch 419 train loss: 0.19155137240886688 val loss: 2.7957923412323\n",
      "epoch 420 train loss: 0.19171498715877533 val loss: 2.6389451026916504\n",
      "epoch 421 train loss: 0.19164277613162994 val loss: 2.9473719596862793\n",
      "epoch 422 train loss: 0.19148723781108856 val loss: 2.9887137413024902\n",
      "epoch 423 train loss: 0.19168928265571594 val loss: 3.0628268718719482\n",
      "epoch 424 train loss: 0.19134388864040375 val loss: 3.6950607299804688\n",
      "epoch 425 train loss: 0.19168390333652496 val loss: 2.933255195617676\n",
      "epoch 426 train loss: 0.19128362834453583 val loss: 3.1031103134155273\n",
      "epoch 427 train loss: 0.19162783026695251 val loss: 3.1210591793060303\n",
      "epoch 428 train loss: 0.1912919580936432 val loss: 2.8489842414855957\n",
      "epoch 429 train loss: 0.19153942167758942 val loss: 2.8394851684570312\n",
      "epoch 430 train loss: 0.19133694469928741 val loss: 2.7017662525177\n",
      "epoch 431 train loss: 0.1914394348859787 val loss: 3.1386733055114746\n",
      "epoch 432 train loss: 0.19138392806053162 val loss: 2.9137158393859863\n",
      "epoch 433 train loss: 0.19135400652885437 val loss: 3.1184608936309814\n",
      "epoch 434 train loss: 0.1914113312959671 val loss: 3.0771076679229736\n",
      "epoch 435 train loss: 0.1913001388311386 val loss: 2.64072847366333\n",
      "epoch 436 train loss: 0.19141143560409546 val loss: 2.9342093467712402\n",
      "epoch 437 train loss: 0.19128015637397766 val loss: 3.088094472885132\n",
      "epoch 438 train loss: 0.19138914346694946 val loss: 3.758824348449707\n",
      "epoch 439 train loss: 0.1912859082221985 val loss: 2.9454185962677\n",
      "epoch 440 train loss: 0.19135607779026031 val loss: 2.804417848587036\n",
      "epoch 441 train loss: 0.1913035660982132 val loss: 2.9147536754608154\n",
      "epoch 442 train loss: 0.1913227140903473 val loss: 2.907984733581543\n",
      "epoch 443 train loss: 0.1913197785615921 val loss: 3.11006498336792\n",
      "epoch 444 train loss: 0.19129711389541626 val loss: 2.4992306232452393\n",
      "epoch 445 train loss: 0.1913270652294159 val loss: 3.050057888031006\n",
      "epoch 446 train loss: 0.19128316640853882 val loss: 3.7655258178710938\n",
      "epoch 447 train loss: 0.19132375717163086 val loss: 1.6168370246887207\n",
      "epoch 448 train loss: 0.19128058850765228 val loss: 3.7369048595428467\n",
      "epoch 449 train loss: 0.1913125365972519 val loss: 3.0252625942230225\n",
      "epoch 450 train loss: 0.19128552079200745 val loss: 3.0648162364959717\n",
      "epoch 451 train loss: 0.19129876792430878 val loss: 3.0720953941345215\n",
      "epoch 452 train loss: 0.19129276275634766 val loss: 2.9219210147857666\n",
      "epoch 453 train loss: 0.1912873536348343 val loss: 2.850162982940674\n",
      "epoch 454 train loss: 0.19129739701747894 val loss: 2.7684006690979004\n",
      "epoch 455 train loss: 0.19128121435642242 val loss: 2.9474258422851562\n",
      "epoch 456 train loss: 0.19129705429077148 val loss: 2.973227024078369\n",
      "epoch 457 train loss: 0.1912802755832672 val loss: 2.8211450576782227\n",
      "epoch 458 train loss: 0.1912926733493805 val loss: 2.7593870162963867\n",
      "epoch 459 train loss: 0.19128267467021942 val loss: 3.3045167922973633\n",
      "epoch 460 train loss: 0.19128674268722534 val loss: 3.383092164993286\n",
      "epoch 461 train loss: 0.19128556549549103 val loss: 2.6202046871185303\n",
      "epoch 462 train loss: 0.19128192961215973 val loss: 3.100926399230957\n",
      "epoch 463 train loss: 0.1912870705127716 val loss: 3.307379722595215\n",
      "epoch 464 train loss: 0.19127978384494781 val loss: 3.0598573684692383\n",
      "epoch 465 train loss: 0.1912863403558731 val loss: 2.762561798095703\n",
      "epoch 466 train loss: 0.19128009676933289 val loss: 2.707768440246582\n",
      "epoch 467 train loss: 0.1912841498851776 val loss: 2.7619597911834717\n",
      "epoch 468 train loss: 0.1912815421819687 val loss: 3.3099920749664307\n",
      "epoch 469 train loss: 0.19128169119358063 val loss: 2.7603700160980225\n",
      "epoch 470 train loss: 0.19128276407718658 val loss: 3.103550672531128\n",
      "epoch 471 train loss: 0.1912800818681717 val loss: 2.513629198074341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 472 train loss: 0.19128291308879852 val loss: 2.968806743621826\n",
      "epoch 473 train loss: 0.19127969443798065 val loss: 3.068676233291626\n",
      "epoch 474 train loss: 0.19128207862377167 val loss: 2.9709246158599854\n",
      "epoch 475 train loss: 0.19128023087978363 val loss: 3.1076765060424805\n",
      "epoch 476 train loss: 0.19128084182739258 val loss: 2.5099093914031982\n",
      "epoch 477 train loss: 0.19128090143203735 val loss: 2.8126180171966553\n",
      "epoch 478 train loss: 0.19127991795539856 val loss: 3.0749223232269287\n",
      "epoch 479 train loss: 0.19128113985061646 val loss: 3.0772974491119385\n",
      "epoch 480 train loss: 0.19127964973449707 val loss: 3.075373411178589\n",
      "epoch 481 train loss: 0.19128088653087616 val loss: 3.0735256671905518\n",
      "epoch 482 train loss: 0.19127987325191498 val loss: 2.7136118412017822\n",
      "epoch 483 train loss: 0.1912802904844284 val loss: 3.1291422843933105\n",
      "epoch 484 train loss: 0.19128026068210602 val loss: 2.8403828144073486\n",
      "epoch 485 train loss: 0.191279798746109 val loss: 3.0570363998413086\n",
      "epoch 486 train loss: 0.19128037989139557 val loss: 2.8063652515411377\n",
      "epoch 487 train loss: 0.19127969443798065 val loss: 2.9681742191314697\n",
      "epoch 488 train loss: 0.19128020107746124 val loss: 3.022289752960205\n",
      "epoch 489 train loss: 0.19127975404262543 val loss: 3.30892276763916\n",
      "epoch 490 train loss: 0.19127987325191498 val loss: 2.8985140323638916\n",
      "epoch 491 train loss: 0.19127993285655975 val loss: 2.899017333984375\n",
      "epoch 492 train loss: 0.19127969443798065 val loss: 3.022311210632324\n",
      "epoch 493 train loss: 0.19127999246120453 val loss: 2.899271249771118\n",
      "epoch 494 train loss: 0.19127966463565826 val loss: 2.839541435241699\n",
      "epoch 495 train loss: 0.19127993285655975 val loss: 3.6446006298065186\n",
      "epoch 496 train loss: 0.19127973914146423 val loss: 2.5116987228393555\n",
      "epoch 497 train loss: 0.19127973914146423 val loss: 2.7594447135925293\n",
      "epoch 498 train loss: 0.1912798136472702 val loss: 2.806013822555542\n",
      "epoch 499 train loss: 0.19127963483333588 val loss: 2.952860116958618\n",
      "epoch 500 train loss: 0.1912798434495926 val loss: 3.6439783573150635\n",
      "epoch 501 train loss: 0.19127964973449707 val loss: 3.0713706016540527\n",
      "epoch 502 train loss: 0.19127975404262543 val loss: 3.3086345195770264\n",
      "epoch 503 train loss: 0.19127970933914185 val loss: 2.9525580406188965\n",
      "epoch 504 train loss: 0.19127969443798065 val loss: 2.9516847133636475\n",
      "epoch 505 train loss: 0.19127972424030304 val loss: 2.8347573280334473\n",
      "epoch 506 train loss: 0.19127964973449707 val loss: 2.952765941619873\n",
      "epoch 507 train loss: 0.19127969443798065 val loss: 2.8344807624816895\n",
      "epoch 508 train loss: 0.19127967953681946 val loss: 2.8060176372528076\n",
      "epoch 509 train loss: 0.19127967953681946 val loss: 3.1305460929870605\n",
      "epoch 510 train loss: 0.19127969443798065 val loss: 3.1055727005004883\n",
      "epoch 511 train loss: 0.19127966463565826 val loss: 3.0762643814086914\n",
      "epoch 512 train loss: 0.19127967953681946 val loss: 3.376155376434326\n",
      "epoch 513 train loss: 0.19127966463565826 val loss: 2.8408894538879395\n",
      "epoch 514 train loss: 0.19127966463565826 val loss: 3.6437501907348633\n",
      "epoch 515 train loss: 0.19127966463565826 val loss: 3.6438944339752197\n",
      "epoch 516 train loss: 0.19127969443798065 val loss: 3.3082215785980225\n",
      "epoch 517 train loss: 0.19127969443798065 val loss: 3.071774959564209\n",
      "epoch 518 train loss: 0.19127961993217468 val loss: 2.888854503631592\n",
      "epoch 519 train loss: 0.19127963483333588 val loss: 2.8125317096710205\n",
      "epoch 520 train loss: 0.19127963483333588 val loss: 2.924689292907715\n",
      "epoch 521 train loss: 0.19127966463565826 val loss: 2.840385913848877\n",
      "epoch 522 train loss: 0.19127967953681946 val loss: 2.834566354751587\n",
      "epoch 523 train loss: 0.19127961993217468 val loss: 2.8998563289642334\n",
      "epoch 524 train loss: 0.19127963483333588 val loss: 2.83449387550354\n",
      "epoch 525 train loss: 0.19127964973449707 val loss: 3.375892162322998\n",
      "epoch 526 train loss: 0.19127963483333588 val loss: 2.8347957134246826\n",
      "epoch 527 train loss: 0.19127966463565826 val loss: 2.6519341468811035\n",
      "epoch 528 train loss: 0.19127961993217468 val loss: 2.9693963527679443\n",
      "epoch 529 train loss: 0.19127964973449707 val loss: 2.7641327381134033\n",
      "epoch 530 train loss: 0.19127966463565826 val loss: 3.7514443397521973\n",
      "epoch 531 train loss: 0.19127964973449707 val loss: 3.7511138916015625\n",
      "epoch 532 train loss: 0.19127964973449707 val loss: 3.105053186416626\n",
      "epoch 533 train loss: 0.19127963483333588 val loss: 2.8998095989227295\n",
      "epoch 534 train loss: 0.19127966463565826 val loss: 3.10537052154541\n",
      "epoch 535 train loss: 0.19127963483333588 val loss: 2.889099359512329\n",
      "epoch 536 train loss: 0.19127963483333588 val loss: 3.0767478942871094\n",
      "epoch 537 train loss: 0.1912795901298523 val loss: 2.840646266937256\n",
      "epoch 538 train loss: 0.19127963483333588 val loss: 2.511237144470215\n",
      "epoch 539 train loss: 0.19127964973449707 val loss: 2.834613561630249\n",
      "epoch 540 train loss: 0.19127963483333588 val loss: 3.130079507827759\n",
      "epoch 541 train loss: 0.1912795752286911 val loss: 2.8129005432128906\n",
      "epoch 542 train loss: 0.19127963483333588 val loss: 3.1053731441497803\n",
      "epoch 543 train loss: 0.19127966463565826 val loss: 2.5113868713378906\n",
      "epoch 544 train loss: 0.19127961993217468 val loss: 2.7117486000061035\n",
      "epoch 545 train loss: 0.19127966463565826 val loss: 3.3759536743164062\n",
      "epoch 546 train loss: 0.19127963483333588 val loss: 1.6166635751724243\n",
      "epoch 547 train loss: 0.19127964973449707 val loss: 2.764026165008545\n",
      "epoch 548 train loss: 0.19127961993217468 val loss: 1.6165943145751953\n",
      "epoch 549 train loss: 0.19127961993217468 val loss: 2.8062829971313477\n",
      "epoch 550 train loss: 0.1912796050310135 val loss: 2.840679168701172\n",
      "epoch 551 train loss: 0.19127961993217468 val loss: 2.651843786239624\n",
      "epoch 552 train loss: 0.1912795901298523 val loss: 2.9695348739624023\n",
      "epoch 553 train loss: 0.19127963483333588 val loss: 3.1051061153411865\n",
      "epoch 554 train loss: 0.19127964973449707 val loss: 2.8129773139953613\n",
      "epoch 555 train loss: 0.19127963483333588 val loss: 3.7512216567993164\n",
      "epoch 556 train loss: 0.19127964973449707 val loss: 3.75130295753479\n",
      "epoch 557 train loss: 0.1912796050310135 val loss: 3.063833713531494\n",
      "epoch 558 train loss: 0.19127963483333588 val loss: 3.1050145626068115\n",
      "epoch 559 train loss: 0.19127966463565826 val loss: 2.813014030456543\n",
      "epoch 560 train loss: 0.19127964973449707 val loss: 2.9521000385284424\n",
      "epoch 561 train loss: 0.19127966463565826 val loss: 2.8996288776397705\n",
      "epoch 562 train loss: 0.19127966463565826 val loss: 3.021911144256592\n",
      "epoch 563 train loss: 0.1912796050310135 val loss: 2.8346078395843506\n",
      "epoch 564 train loss: 0.19127963483333588 val loss: 2.7639925479888916\n",
      "epoch 565 train loss: 0.19127963483333588 val loss: 2.8345839977264404\n",
      "epoch 566 train loss: 0.19127963483333588 val loss: 1.6166415214538574\n",
      "epoch 567 train loss: 0.19127963483333588 val loss: 2.7640061378479004\n",
      "epoch 568 train loss: 0.19127963483333588 val loss: 2.7640058994293213\n",
      "epoch 569 train loss: 0.19127963483333588 val loss: 3.1050631999969482\n",
      "epoch 570 train loss: 0.19127963483333588 val loss: 3.0219104290008545\n",
      "epoch 571 train loss: 0.19127963483333588 val loss: 3.0219004154205322\n",
      "epoch 572 train loss: 0.1912796050310135 val loss: 3.644193410873413\n",
      "epoch 573 train loss: 0.19127963483333588 val loss: 2.840646982192993\n",
      "epoch 574 train loss: 0.19127961993217468 val loss: 2.7117817401885986\n",
      "epoch 575 train loss: 0.19127963483333588 val loss: 2.9695303440093994\n",
      "epoch 576 train loss: 0.1912796050310135 val loss: 3.7512564659118652\n",
      "epoch 577 train loss: 0.19127964973449707 val loss: 3.0219075679779053\n",
      "epoch 578 train loss: 0.1912796050310135 val loss: 3.1050479412078857\n",
      "epoch 579 train loss: 0.19127966463565826 val loss: 2.651892900466919\n",
      "epoch 580 train loss: 0.19127964973449707 val loss: 2.511331081390381\n",
      "epoch 581 train loss: 0.19127963483333588 val loss: 2.812987804412842\n",
      "epoch 582 train loss: 0.19127964973449707 val loss: 3.130047082901001\n",
      "epoch 583 train loss: 0.19127963483333588 val loss: 3.021909236907959\n",
      "epoch 584 train loss: 0.19127963483333588 val loss: 3.7512266635894775\n",
      "epoch 585 train loss: 0.19127966463565826 val loss: 2.8406436443328857\n",
      "epoch 586 train loss: 0.19127966463565826 val loss: 2.8345937728881836\n",
      "epoch 587 train loss: 0.1912796050310135 val loss: 2.9520821571350098\n",
      "epoch 588 train loss: 0.19127963483333588 val loss: 3.0219128131866455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 589 train loss: 0.19127964973449707 val loss: 2.8996646404266357\n",
      "epoch 590 train loss: 0.1912795901298523 val loss: 3.1072490215301514\n",
      "epoch 591 train loss: 0.19127966463565826 val loss: 3.1050469875335693\n",
      "epoch 592 train loss: 0.19127966463565826 val loss: 2.9246022701263428\n",
      "epoch 593 train loss: 0.19127966463565826 val loss: 2.899657964706421\n",
      "epoch 594 train loss: 0.19127961993217468 val loss: 2.5113368034362793\n",
      "epoch 595 train loss: 0.19127963483333588 val loss: 2.8891823291778564\n",
      "epoch 596 train loss: 0.19127966463565826 val loss: 3.7512333393096924\n",
      "epoch 597 train loss: 0.19127964973449707 val loss: 3.0638344287872314\n",
      "epoch 598 train loss: 0.19127964973449707 val loss: 3.0719070434570312\n",
      "epoch 599 train loss: 0.19127963483333588 val loss: 2.924597978591919\n",
      "epoch 600 train loss: 0.19127964973449707 val loss: 2.8891777992248535\n",
      "epoch 601 train loss: 0.19127964973449707 val loss: 2.929029941558838\n",
      "epoch 602 train loss: 0.19127964973449707 val loss: 2.7117927074432373\n",
      "epoch 603 train loss: 0.19127961993217468 val loss: 2.95209002494812\n",
      "epoch 604 train loss: 0.19127961993217468 val loss: 1.616629958152771\n",
      "epoch 605 train loss: 0.19127964973449707 val loss: 3.021912097930908\n",
      "epoch 606 train loss: 0.19127964973449707 val loss: 3.0219099521636963\n",
      "epoch 607 train loss: 0.19127966463565826 val loss: 2.9520933628082275\n",
      "epoch 608 train loss: 0.19127963483333588 val loss: 2.899662971496582\n",
      "epoch 609 train loss: 0.19127964973449707 val loss: 3.1300320625305176\n",
      "epoch 610 train loss: 0.19127964973449707 val loss: 3.3760268688201904\n",
      "epoch 611 train loss: 0.19127964973449707 val loss: 1.6166287660598755\n",
      "epoch 612 train loss: 0.19127966463565826 val loss: 3.376021385192871\n",
      "epoch 613 train loss: 0.19127964973449707 val loss: 3.0638341903686523\n",
      "epoch 614 train loss: 0.19127963483333588 val loss: 2.8996636867523193\n",
      "epoch 615 train loss: 0.19127966463565826 val loss: 2.9290220737457275\n",
      "epoch 616 train loss: 0.19127963483333588 val loss: 2.9290237426757812\n",
      "epoch 617 train loss: 0.19127964973449707 val loss: 3.307945966720581\n",
      "epoch 618 train loss: 0.19127964973449707 val loss: 2.8062446117401123\n",
      "epoch 619 train loss: 0.19127966463565826 val loss: 2.6168901920318604\n",
      "epoch 620 train loss: 0.19127963483333588 val loss: 1.6166281700134277\n",
      "epoch 621 train loss: 0.1912796050310135 val loss: 3.10725736618042\n",
      "epoch 622 train loss: 0.1912796050310135 val loss: 3.105043888092041\n",
      "epoch 623 train loss: 0.19127963483333588 val loss: 2.812436580657959\n",
      "epoch 624 train loss: 0.19127963483333588 val loss: 2.9290242195129395\n",
      "epoch 625 train loss: 0.19127961993217468 val loss: 2.840637683868408\n",
      "epoch 626 train loss: 0.19127961993217468 val loss: 3.1072585582733154\n",
      "epoch 627 train loss: 0.19127964973449707 val loss: 1.6166270971298218\n",
      "epoch 628 train loss: 0.19127963483333588 val loss: 3.02191162109375\n",
      "epoch 629 train loss: 0.19127961993217468 val loss: 2.8124356269836426\n",
      "epoch 630 train loss: 0.19127964973449707 val loss: 3.0719008445739746\n",
      "epoch 631 train loss: 0.19127963483333588 val loss: 3.376023054122925\n",
      "epoch 632 train loss: 0.19127963483333588 val loss: 3.07190203666687\n",
      "epoch 633 train loss: 0.19127967953681946 val loss: 2.952091693878174\n",
      "epoch 634 train loss: 0.19127966463565826 val loss: 3.307941436767578\n",
      "epoch 635 train loss: 0.19127961993217468 val loss: 2.760284900665283\n",
      "epoch 636 train loss: 0.19127966463565826 val loss: 3.1300361156463623\n",
      "epoch 637 train loss: 0.19127967953681946 val loss: 3.0766515731811523\n",
      "epoch 638 train loss: 0.19127963483333588 val loss: 2.9245991706848145\n",
      "epoch 639 train loss: 0.19127963483333588 val loss: 3.107257127761841\n",
      "epoch 640 train loss: 0.19127963483333588 val loss: 3.107255458831787\n",
      "epoch 641 train loss: 0.1912796050310135 val loss: 3.7512359619140625\n",
      "epoch 642 train loss: 0.19127961993217468 val loss: 3.1054112911224365\n",
      "epoch 643 train loss: 0.19127963483333588 val loss: 2.7117927074432373\n",
      "epoch 644 train loss: 0.1912796050310135 val loss: 2.7640199661254883\n",
      "epoch 645 train loss: 0.19127961993217468 val loss: 2.840637445449829\n",
      "epoch 646 train loss: 0.19127961993217468 val loss: 3.3079428672790527\n",
      "epoch 647 train loss: 0.19127963483333588 val loss: 3.7512362003326416\n",
      "epoch 648 train loss: 0.19127964973449707 val loss: 2.969520330429077\n",
      "epoch 649 train loss: 0.1912796050310135 val loss: 3.7512357234954834\n",
      "epoch 650 train loss: 0.19127961993217468 val loss: 2.7117927074432373\n",
      "epoch 651 train loss: 0.19127964973449707 val loss: 3.307943820953369\n",
      "epoch 652 train loss: 0.19127966463565826 val loss: 2.81299090385437\n",
      "epoch 653 train loss: 0.19127966463565826 val loss: 2.760284423828125\n",
      "epoch 654 train loss: 0.19127963483333588 val loss: 2.6168901920318604\n",
      "epoch 655 train loss: 0.19127964973449707 val loss: 2.812990665435791\n",
      "epoch 656 train loss: 0.19127961993217468 val loss: 3.7512357234954834\n",
      "epoch 657 train loss: 0.1912796050310135 val loss: 2.969519853591919\n",
      "epoch 658 train loss: 0.19127963483333588 val loss: 3.0550754070281982\n",
      "epoch 659 train loss: 0.19127961993217468 val loss: 3.076652765274048\n",
      "epoch 660 train loss: 0.1912796050310135 val loss: 3.1072568893432617\n",
      "epoch 661 train loss: 0.19127961993217468 val loss: 2.8129899501800537\n",
      "epoch 662 train loss: 0.19127961993217468 val loss: 2.8996617794036865\n",
      "epoch 663 train loss: 0.19127963483333588 val loss: 2.924598217010498\n",
      "epoch 664 train loss: 0.19127964973449707 val loss: 2.6168906688690186\n",
      "epoch 665 train loss: 0.19127964973449707 val loss: 2.8062450885772705\n",
      "epoch 666 train loss: 0.19127966463565826 val loss: 3.307943105697632\n",
      "epoch 667 train loss: 0.19127963483333588 val loss: 2.929022789001465\n",
      "epoch 668 train loss: 0.19127964973449707 val loss: 2.8129897117614746\n",
      "epoch 669 train loss: 0.19127963483333588 val loss: 2.8124351501464844\n",
      "epoch 670 train loss: 0.19127963483333588 val loss: 1.616627812385559\n",
      "epoch 671 train loss: 0.19127964973449707 val loss: 2.9245975017547607\n",
      "epoch 672 train loss: 0.19127963483333588 val loss: 3.0766520500183105\n",
      "epoch 673 train loss: 0.19127964973449707 val loss: 3.6441738605499268\n",
      "epoch 674 train loss: 0.19127964973449707 val loss: 3.10504412651062\n",
      "epoch 675 train loss: 0.1912796050310135 val loss: 2.812434196472168\n",
      "epoch 676 train loss: 0.19127963483333588 val loss: 3.0766525268554688\n",
      "epoch 677 train loss: 0.19127963483333588 val loss: 2.760284662246704\n",
      "epoch 678 train loss: 0.19127963483333588 val loss: 2.840637445449829\n",
      "epoch 679 train loss: 0.19127964973449707 val loss: 2.616889476776123\n",
      "epoch 680 train loss: 0.19127963483333588 val loss: 2.760283946990967\n",
      "epoch 681 train loss: 0.19127961993217468 val loss: 2.969520330429077\n",
      "epoch 682 train loss: 0.19127964973449707 val loss: 2.8996622562408447\n",
      "epoch 683 train loss: 0.19127963483333588 val loss: 3.105410575866699\n",
      "epoch 684 train loss: 0.19127963483333588 val loss: 2.6168899536132812\n",
      "epoch 685 train loss: 0.19127963483333588 val loss: 3.3760247230529785\n",
      "epoch 686 train loss: 0.19127963483333588 val loss: 2.511335849761963\n",
      "epoch 687 train loss: 0.19127963483333588 val loss: 3.055075168609619\n",
      "epoch 688 train loss: 0.19127963483333588 val loss: 2.651878595352173\n",
      "epoch 689 train loss: 0.19127966463565826 val loss: 2.6168901920318604\n",
      "epoch 690 train loss: 0.19127964973449707 val loss: 2.760284185409546\n",
      "epoch 691 train loss: 0.19127963483333588 val loss: 3.376023769378662\n",
      "epoch 692 train loss: 0.19127963483333588 val loss: 2.812434434890747\n",
      "epoch 693 train loss: 0.19127964973449707 val loss: 2.7640202045440674\n",
      "epoch 694 train loss: 0.19127963483333588 val loss: 2.840636968612671\n",
      "epoch 695 train loss: 0.19127966463565826 val loss: 3.0766522884368896\n",
      "epoch 696 train loss: 0.19127963483333588 val loss: 2.8996617794036865\n",
      "epoch 697 train loss: 0.19127966463565826 val loss: 2.840636730194092\n",
      "epoch 698 train loss: 0.19127966463565826 val loss: 3.1072561740875244\n",
      "epoch 699 train loss: 0.19127966463565826 val loss: 3.063832998275757\n",
      "epoch 700 train loss: 0.19127967953681946 val loss: 3.0719001293182373\n",
      "epoch 701 train loss: 0.19127964973449707 val loss: 2.7602837085723877\n",
      "epoch 702 train loss: 0.19127964973449707 val loss: 2.889177083969116\n",
      "epoch 703 train loss: 0.19127964973449707 val loss: 2.840636730194092\n",
      "epoch 704 train loss: 0.19127966463565826 val loss: 2.840636730194092\n",
      "epoch 705 train loss: 0.19127964973449707 val loss: 2.95209002494812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 706 train loss: 0.1912796050310135 val loss: 2.9245975017547607\n",
      "epoch 707 train loss: 0.1912796050310135 val loss: 3.0766518115997314\n",
      "epoch 708 train loss: 0.19127961993217468 val loss: 3.0766518115997314\n",
      "epoch 709 train loss: 0.19127963483333588 val loss: 2.8406362533569336\n",
      "epoch 710 train loss: 0.19127963483333588 val loss: 1.6166273355484009\n",
      "epoch 711 train loss: 0.1912796050310135 val loss: 2.806244373321533\n",
      "epoch 712 train loss: 0.1912795901298523 val loss: 3.1054089069366455\n",
      "epoch 713 train loss: 0.19127963483333588 val loss: 2.8129897117614746\n",
      "epoch 714 train loss: 0.1912796050310135 val loss: 2.840636968612671\n",
      "epoch 715 train loss: 0.19127963483333588 val loss: 3.130033254623413\n",
      "epoch 716 train loss: 0.19127963483333588 val loss: 1.6166282892227173\n",
      "epoch 717 train loss: 0.19127963483333588 val loss: 2.9290215969085693\n",
      "epoch 718 train loss: 0.19127961993217468 val loss: 2.8129892349243164\n",
      "epoch 719 train loss: 0.19127966463565826 val loss: 2.92459774017334\n",
      "epoch 720 train loss: 0.19127963483333588 val loss: 3.1054086685180664\n",
      "epoch 721 train loss: 0.19127966463565826 val loss: 3.0718986988067627\n",
      "epoch 722 train loss: 0.19127963483333588 val loss: 3.751236915588379\n",
      "epoch 723 train loss: 0.19127966463565826 val loss: 3.1054089069366455\n",
      "epoch 724 train loss: 0.19127963483333588 val loss: 2.95209002494812\n",
      "epoch 725 train loss: 0.19127966463565826 val loss: 2.7640204429626465\n",
      "epoch 726 train loss: 0.19127966463565826 val loss: 3.1072561740875244\n",
      "epoch 727 train loss: 0.1912796050310135 val loss: 2.929022789001465\n",
      "epoch 728 train loss: 0.19127970933914185 val loss: 2.929023504257202\n",
      "epoch 729 train loss: 0.19127963483333588 val loss: 3.0766525268554688\n",
      "epoch 730 train loss: 0.19127963483333588 val loss: 2.764019727706909\n",
      "epoch 731 train loss: 0.19127964973449707 val loss: 2.8891775608062744\n",
      "epoch 732 train loss: 0.19127963483333588 val loss: 2.5113351345062256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-29a3d0db75ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {t+1} train loss: {train_loss} val loss: {test_loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-b52045b9ae75>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# convert from torch to numpy again, X,y are of type torch.tensor, so that we could perform normalisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-7741f49c796d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcomplex_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmt_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplex_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mt_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmt_code\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogical_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   2755\u001b[0m         \u001b[0;31m# We do not pass dtype to ensure that the Series constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m         \u001b[0;31m#  does inference in the case where `result` has object-dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfrom_array\u001b[0;34m(cls, array, index)\u001b[0m\n\u001b[1;32m   1568\u001b[0m         \"\"\"\n\u001b[1;32m   1569\u001b[0m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_setstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     ):\n\u001b[0;32m-> 1539\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1540\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = BindingSiteDataset('train_df.csv', 'test_db/single_near_mutant_CYP3A4_db.csv')\n",
    "val_data = BindingSiteDataset('val_df.csv', 'test_db/single_near_mutant_CYP3A4_db.csv')\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=60, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=30, shuffle=True)\n",
    "\n",
    "model = SimpleNNRegression()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss = test_loop(val_dataloader, model, loss_fn)\n",
    "    print(f'epoch {t+1} train loss: {train_loss} val loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2.2300]]), tensor([[18.7000]]), tensor([[17.5000]]), tensor([[6.7900]]), tensor([[13.]]), tensor([[14.4000]]), tensor([[16.4000]]), tensor([[229.]]), tensor([[36.]]), tensor([[29.]]), tensor([[10.4000]]), tensor([[7.1040]]), tensor([[0.5100]]), tensor([[28.]]), tensor([[0.1200]]), tensor([[4.7000]]), tensor([[4.3000]]), tensor([[17.3000]]), tensor([[3.6000]]), tensor([[3.2000]]), tensor([[4.3000]]), tensor([[9.1000]]), tensor([[1.7700]]), tensor([[28.]]), tensor([[16.6000]]), tensor([[4.5000]]), tensor([[401.]]), tensor([[14.9000]]), tensor([[5.5000]]), tensor([[3.9600]]), tensor([[8.2620]]), tensor([[4.1000]]), tensor([[1.9000]]), tensor([[47.6000]]), tensor([[0.8200]]), tensor([[11.]]), tensor([[3.0900]]), tensor([[23.8000]]), tensor([[20.9800]]), tensor([[11.2000]]), tensor([[6.8000]]), tensor([[1.6500]]), tensor([[9.5700]]), tensor([[30.2000]]), tensor([[30.3000]]), tensor([[6.7000]]), tensor([[86.]]), tensor([[6.4000]]), tensor([[1.1300]]), tensor([[13.5000]]), tensor([[3.8000]]), tensor([[22.8000]]), tensor([[17.]]), tensor([[2.4310]]), tensor([[3.0400]]), tensor([[22.]]), tensor([[5.1000]]), tensor([[3.7000]]), tensor([[5.1200]]), tensor([[8.1300]])]\n",
      "\n",
      "\n",
      "[tensor([[0.4200]]), tensor([[2.1000]]), tensor([[22.6000]]), tensor([[6.3000]]), tensor([[1.3100]]), tensor([[9.]]), tensor([[4.5000]]), tensor([[19.5000]]), tensor([[16.]]), tensor([[4.3000]]), tensor([[1.5000]]), tensor([[2.2000]]), tensor([[1.6700]]), tensor([[2.6500]]), tensor([[2.6000]]), tensor([[16.7400]]), tensor([[7.5000]]), tensor([[3.4000]]), tensor([[38.7000]]), tensor([[1.4100]]), tensor([[9.9700]]), tensor([[4.3000]]), tensor([[24.7900]]), tensor([[45.3000]]), tensor([[55.5000]]), tensor([[23.5000]]), tensor([[8.0700]]), tensor([[7.1000]]), tensor([[26.]]), tensor([[10.8600]]), tensor([[23.9000]])]\n"
     ]
    }
   ],
   "source": [
    "print(train_truths)\n",
    "print('\\n')\n",
    "print(val_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howc/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(train_truths, hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(val_truths, hist=True, kde=False, \n",
    "             bins=int(180/5), color = 'blue',\n",
    "             hist_kws={'edgecolor':'black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
